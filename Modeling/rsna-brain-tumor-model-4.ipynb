{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n#         print(os.path.join(dirname, filename))\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"36f4f43f-d564-494a-b856-f95332b19a88","_cell_guid":"c0aa58c9-9495-464e-9b33-c2d53ddc92e5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2021-09-21T18:27:07.336899Z","iopub.execute_input":"2021-09-21T18:27:07.337822Z","iopub.status.idle":"2021-09-21T18:28:26.147543Z","shell.execute_reply.started":"2021-09-21T18:27:07.337766Z","shell.execute_reply":"2021-09-21T18:28:26.146690Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys \nimport glob\nimport time\nimport gc\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:29:53.681779Z","iopub.execute_input":"2021-09-21T18:29:53.682070Z","iopub.status.idle":"2021-09-21T18:29:56.082777Z","shell.execute_reply.started":"2021-09-21T18:29:53.682037Z","shell.execute_reply":"2021-09-21T18:29:56.081784Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#declare variables\ndata_directory =  Path('../input/processed-voxels/voxels')\n\nscan_types = ['T2w','T1wCE', 'T1w', 'FLAIR']","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:30:08.877009Z","iopub.execute_input":"2021-09-21T18:30:08.877824Z","iopub.status.idle":"2021-09-21T18:30:08.882257Z","shell.execute_reply.started":"2021-09-21T18:30:08.877782Z","shell.execute_reply":"2021-09-21T18:30:08.881069Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#training labels\ntrain_labels = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')\ntrain_labels = train_labels[~train_labels['BraTS21ID'].isin([109, 123, 709])]","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:30:28.763513Z","iopub.execute_input":"2021-09-21T18:30:28.763816Z","iopub.status.idle":"2021-09-21T18:30:28.789240Z","shell.execute_reply.started":"2021-09-21T18:30:28.763784Z","shell.execute_reply":"2021-09-21T18:30:28.788610Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#creating training & validation data\ndf_train, df_valid = train_test_split(\n    train_labels, \n    test_size=0.2, \n    random_state=42, \n    stratify=train_labels[\"MGMT_value\"])\n\nprint(df_train.shape)\nprint(df_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:30:38.422178Z","iopub.execute_input":"2021-09-21T18:30:38.423021Z","iopub.status.idle":"2021-09-21T18:30:38.433550Z","shell.execute_reply.started":"2021-09-21T18:30:38.422980Z","shell.execute_reply":"2021-09-21T18:30:38.432714Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(465, 2)\n(117, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"#apply mclache using github repo https://github.com/VincentStimper/mclahe\n\nslice_ind = slice\n\ndef slice(input, begin, size):\n    \"\"\"\n    Slice an array using a iterable start and length vector\n    :param input: Input array to be sliced\n    :param begin: Vector of indices where to start slicing\n    :param size: Vector of slice lengths\n    :return: Sliced array\n    \"\"\"\n\n    slices = tuple(slice_ind(b, b + s) for b, s in zip(begin, size))\n    return input[slices]\n\n\ndef batch_gather(params, indices, axis):\n    \"\"\"\n    Gather slices from `params` according to `indices` with leading batch dims.\n    This operation assumes that the leading dimensions of `indices` are dense,\n    and the gathers on the axis corresponding to the last dimension of `indices`.\n    More concretely it computes:\n    `result[i1, ..., in, j1, ..., jm, k1, ...., kl] = params[i1, ..., in, indices[i1, ..., in, j1, ..., jm], k1, ..., kl]`\n    Therefore `params` should be a Tensor of shape [A1, ..., AN, C0, B1, ..., BM],\n    `indices` should be a Tensor of shape [A1, ..., AN, C1, ..., CK] and `result` will be\n    a Tensor of size `[A1, ..., AN, C1, ..., CK, B1, ..., BM]`.\n    Args:\n      params: The array from which to gather values.\n      indices: Must be one of the following types: int32, int64. Index\n          array. Must be in range `[0, params.shape[axis]`, where `axis` is the\n          last dimension of `indices` itself.\n      axis: Must be one of the following types: int32, int64. The axis\n            in `params` to gather `indices` from.\n    Returns:\n      An array. Has the same type as `params`.\n    \"\"\"\n\n    indices_shape = indices.shape\n    params_shape = params.shape\n    ndim = indices.ndim\n    indices_internal = indices.copy()\n\n    # Adapt indices to act on respective batch\n    accum_dim_value = 1\n    for dim in range(axis, 0, -1):\n        dim_value = params_shape[dim - 1]\n        accum_dim_value *= params_shape[dim]\n        dim_indices = np.arange(dim_value)\n        dim_indices *= accum_dim_value\n        dim_shape = [1] * (dim - 1) + [dim_value] + [1] * (ndim - dim)\n        indices_internal += dim_indices.reshape(*dim_shape)\n\n    flat_inner_shape_indices = np.prod(indices_shape[:(axis + 1)])\n    flat_indices = indices_internal.reshape(*((flat_inner_shape_indices,) + indices_shape[(axis + 1):]))\n    outer_shape = params_shape[(axis + 1):]\n    flat_inner_shape_params = np.prod(params_shape[:(axis + 1)])\n\n    flat_params = params.reshape(*((flat_inner_shape_params,) + outer_shape))\n    flat_result = flat_params[flat_indices,...]\n    result = flat_result.reshape(*(indices_shape + outer_shape))\n    return result\n\n\ndef batch_histogram(values, value_range, axis, nbins=100, use_map=False):\n    \"\"\"\n    Computes histogram with fixed width considering batch dimensions\n    :param values: Array containing the values for histogram computation.\n    :param value_range: Shape [2] iterable. values <= value_range[0] will be mapped to\n    hist[0], values >= value_range[1] will be mapped to hist[-1].\n    :param axis: Number of batch dimensions. First axis to apply histogram computation to.\n    :param nbins: Scalar. Number of histogram bins.\n    :param use_map: Flag indicating whether map function is used\n    :return: histogram with batch dimensions.\n    \"\"\"\n\n    # Get shape\n    values_shape = values.shape\n    batch_dim = values_shape[:axis]\n    outer_dim = values_shape[axis:]\n    num_batch = np.prod(batch_dim)\n\n    if use_map:\n        values_reshaped = values.reshape(*((num_batch,) + outer_dim))\n        hist = np.array(list(map(lambda x: np.histogram(x, range=value_range, bins=nbins)[0], values_reshaped)))\n    else:\n        # Normalize\n        values_double = values.astype('double')\n        value_range_double = np.array(value_range).astype('double')\n\n        # Clip values\n        values_norm = (values_double - value_range_double[0]) / (value_range_double[1] - value_range_double[0])\n        values_clip1 = np.maximum(values_norm, 0.5 / nbins)\n        values_clip2 = np.minimum(values_clip1, 1.0 - 0.5 / nbins)\n\n        # Shift values\n        values_shift = values_clip2 + np.arange(num_batch).reshape(*(batch_dim + len(outer_dim) * (1,)))\n\n        # Get histogram\n        hist = np.histogram(values_shift, range=[0, num_batch], bins=num_batch * nbins)[0]\n\n    return hist.reshape(*(batch_dim + (nbins,)))\n\n\nimport numpy as np\nfrom itertools import product\n\ndef mclahe(x, kernel_size=None, n_bins=128, clip_limit=0.01, adaptive_hist_range=False):\n    \"\"\"\n    Contrast limited adaptive histogram equalization\n    :param x: numpy array to which clahe is applied\n    :param kernel_size: tuple of kernel sizes, 1/8 of dimension lengths of x if None\n    :param n_bins: number of bins to be used in the histogram\n    :param clip_limit: relative intensity limit to be ignored in the histogram equalization\n    :param adaptive_hist_range: flag, if true individual range for histogram computation of each block is used\n    :return: numpy array to which clahe was applied, scaled on interval [0, 1]\n    \"\"\"\n\n    if kernel_size is None:\n        kernel_size = tuple(s // 8 for s in x.shape)\n    kernel_size = np.array(kernel_size)\n\n    assert len(kernel_size) == len(x.shape)\n\n    dim = len(x.shape)\n\n    # Normalize data\n    x_min = np.min(x)\n    x_max = np.max(x)\n    x = (x - x_min) / (x_max - x_min)\n\n    # Pad data\n    x_shape = np.array(x.shape)\n    padding_x_length = kernel_size - 1 - ((x_shape - 1) % kernel_size)\n    padding_x = np.column_stack(((padding_x_length + 1) // 2, padding_x_length // 2))\n    padding_hist = np.column_stack((kernel_size // 2, (kernel_size + 1) // 2)) + padding_x\n    x_hist_padded = np.pad(x, padding_hist, 'symmetric')\n\n    x_padded = slice(x_hist_padded, kernel_size // 2, x_shape + padding_x_length)\n\n    # Form blocks used for interpolation\n    n_blocks = np.ceil(np.array(x.shape) / kernel_size).astype(np.int32)\n    new_shape = np.reshape(np.column_stack((n_blocks, kernel_size)), (2 * dim,))\n    perm = tuple(2 * i for i in range(dim)) + tuple(2 * i + 1 for i in range(dim))\n    x_block = np.transpose(x_padded.reshape(*new_shape), perm)\n    shape_x_block = np.concatenate((n_blocks, kernel_size))\n\n    # Form block used for histogram\n    n_blocks_hist = n_blocks + np.ones(dim, dtype=np.int32)\n    new_shape = np.reshape(np.column_stack((n_blocks_hist, kernel_size)), (2 * dim,))\n    perm = tuple(2 * i for i in range(dim)) + tuple(2 * i + 1 for i in range(dim))\n    x_hist = np.transpose(x_hist_padded.reshape(*new_shape), perm)\n\n    # Get maps\n    # Get histogram\n    if adaptive_hist_range:\n        hist_ex_shape = np.concatenate((n_blocks_hist, [1] * dim))\n        x_hist_max = np.max(x_hist, tuple(np.arange(-dim, 0)))\n        x_hist_min = np.min(x_hist, tuple(np.arange(-dim, 0)))\n        x_hist_norm = np.where(x_hist_min == x_hist_max, np.ones_like(x_hist_min), x_hist_max - x_hist_min)\n\n        x_hist_scaled = (x_hist - x_hist_min.reshape(*hist_ex_shape)) / x_hist_norm.reshape(*hist_ex_shape)\n    else:\n        x_hist_scaled = x_hist\n    hist = batch_histogram(x_hist_scaled, [0., 1.], dim, nbins=n_bins).astype(np.float32)\n    # Clip histogram\n    n_to_high = np.sum(np.maximum(hist - np.prod(kernel_size) * clip_limit, 0), -1, keepdims=True)\n    hist_clipped = np.minimum(hist, np.prod(kernel_size) * clip_limit) + n_to_high / n_bins\n    cdf = np.cumsum(hist_clipped, -1)\n    cdf_min = cdf[..., :1]\n    cdf_max = cdf[..., -1:]\n    cdf_norm = np.where(cdf_min == cdf_max, np.ones_like(cdf_max), cdf_max - cdf_min)\n    mapping = (cdf - cdf_min) / cdf_norm\n\n    # Get global hist bins if needed\n    # Compute always as they are needed for both modes\n    bin_edges = np.histogram_bin_edges(x_hist_scaled, range=[0., 1.], bins=n_bins)[1:-1]\n    if not adaptive_hist_range:\n        # Global bins\n        bin_ind = np.digitize(x_block, bin_edges)\n\n    # Loop over maps to compute result\n    res = np.zeros(shape_x_block)\n    inds = [list(i) for i in product([0, 1], repeat=dim)]\n    for ind_map in inds:\n        # Compute bin indices if local bins are used\n        if adaptive_hist_range:\n            # Local bins\n            hist_norm_slice_shape = np.concatenate((n_blocks, [1] * dim))\n            x_hist_min_sub = slice(x_hist_min, ind_map, n_blocks)\n            x_hist_norm_sub = slice(x_hist_norm, ind_map, n_blocks)\n            x_block_scaled = (x_block - x_hist_min_sub.reshape(*hist_norm_slice_shape)) \\\n                             / x_hist_norm_sub.reshape(*hist_norm_slice_shape)\n            bin_ind = np.digitize(x_block_scaled, bin_edges)\n        \n        # Apply map\n        map_slice = slice(mapping, ind_map + [0], list(n_blocks) + [n_bins])\n        mapped_sub = batch_gather(map_slice, bin_ind, dim)\n\n        # Calculate and apply coefficients\n        res_sub = mapped_sub\n        for axis in range(dim):\n            coeff = np.arange(kernel_size[axis], dtype=np.float32) / kernel_size[axis]\n            if kernel_size[axis] % 2 == 0:\n                coeff = 0.5 / kernel_size[axis] + coeff\n            if ind_map[axis] == 0:\n                coeff = 1. - coeff\n            new_shape = [1] * (dim + axis) + [kernel_size[axis]] + [1] * (dim - 1 - axis)\n            coeff = np.reshape(coeff, new_shape)\n            res_sub = coeff * res_sub\n\n        # Update results\n        res = res + res_sub\n\n    # Rescaling\n    res_min, res_max = (np.min(res), np.max(res))\n    res_norm = (res - res_min) / (res_max - res_min)\n\n    # Reshape result\n    new_shape = tuple((axis, axis + dim) for axis in range(dim))\n    new_shape = tuple(j for i in new_shape for j in i)\n    res_transposed = np.transpose(res_norm, new_shape)\n    res_reshaped = res_transposed.reshape(*tuple(n_blocks[axis] * kernel_size[axis] for axis in range(dim)))\n\n    # Recover original size\n    result = slice(res_reshaped, padding_x[:, 0], x.shape)\n\n    return result\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:35:02.497678Z","iopub.execute_input":"2021-09-21T18:35:02.497983Z","iopub.status.idle":"2021-09-21T18:35:02.536721Z","shell.execute_reply.started":"2021-09-21T18:35:02.497953Z","shell.execute_reply":"2021-09-21T18:35:02.535863Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def get_image_plane(data):\n    \n    '''\n    Get Image Plane based on the X & Y\n    coordinates of the scan stored in\n    the metadata of scan\n    \n    '''\n    x1, y1, _, x2, y2, _ = [round(j) for j in data.ImageOrientationPatient]\n    cords = [x1, y1, x2, y2]\n\n    if cords == [1, 0, 0, 0]:\n        return 'Coronal'\n    elif cords == [1, 0, 0, 1]:\n        return 'Axial'\n    elif cords == [0, 1, 0, 0]:\n        return 'Sagittal'\n    else:\n        return 'Unknown'\n    \ndef get_voxel(split, study_id, scan_type):\n    \n    '''\n    Create Voxel for the given MRI scan and\n    reorder & rotate the scan plane to align\n    in the axial plane\n    '''\n    imgs = []\n    dcm_dir = data_root.joinpath(split, study_id, scan_type)\n    dcm_paths = sorted(dcm_dir.glob(\"*.dcm\"), key=lambda x: int(x.stem.split(\"-\")[-1]))\n    positions = []\n    \n    for dcm_path in dcm_paths:\n        img = pydicom.dcmread(str(dcm_path))\n        imgs.append(img.pixel_array)\n        positions.append(img.ImagePositionPatient)\n        \n    plane = get_image_plane(img)\n    voxel = np.stack(imgs)\n    \n    # reorder planes if needed and rotate voxel\n    if plane == \"Coronal\":\n        if positions[0][1] < positions[-1][1]:\n            voxel = voxel[::-1]\n        voxel = voxel.transpose((1, 0, 2))\n    elif plane == \"Sagittal\":\n        if positions[0][0] < positions[-1][0]:\n            voxel = voxel[::-1]\n        voxel = voxel.transpose((1, 2, 0))\n        voxel = np.rot90(voxel, 2, axes=(1, 2))\n    elif plane == \"Axial\":\n        if positions[0][2] > positions[-1][2]:\n            voxel = voxel[::-1]\n        voxel = np.rot90(voxel, 2)\n    else:\n        raise ValueError(f\"Unknown plane {plane}\")\n    return voxel\n\ndef normalize_contrast(voxel):\n    if voxel.sum() == 0:\n        return voxel\n    voxel = voxel - np.min(voxel)\n    voxel = voxel / np.max(voxel)\n    voxel = (voxel * 255).astype(np.uint8)\n    return voxel\n\ndef crop_voxel(voxel):\n    '''\n    Crop voxels by removing columns with \n    mean value = 0\n    '''\n    \n    if voxel.sum() == 0:\n        return voxel\n    keep = (voxel.mean(axis=(0, 1)) > 0)\n    voxel = voxel[:, :, keep]\n    keep = (voxel.mean(axis=(0, 2)) > 0)\n    voxel = voxel[:, keep, :]\n    keep = (voxel.mean(axis=(1, 2)) > 0)\n    voxel = voxel[keep, :, :]\n    return voxel","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:44:28.519698Z","iopub.execute_input":"2021-09-21T18:44:28.520325Z","iopub.status.idle":"2021-09-21T18:44:28.535847Z","shell.execute_reply.started":"2021-09-21T18:44:28.520283Z","shell.execute_reply":"2021-09-21T18:44:28.535236Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def filter_voxel(voxel, filter_thr):\n    voxel_mean = voxel.mean(axis=(1, 2))\n    keep = (voxel_mean > voxel_mean.std()*filter_thr)\n    voxel = voxel[keep, :, :]\n    return voxel\n\ndef resize_voxel(voxel, sz=(64, 256, 256)):\n    output = np.zeros((sz[0], sz[1], sz[2]), dtype=np.uint8)\n    if np.argmax(voxel.shape) == 0:\n        for i, s in enumerate(np.linspace(0, voxel.shape[0] - 1, num=sz[0])):\n            sampled = voxel[int(s), :, :]\n            output[i, :, :] = cv2.resize(sampled, (sz[2], sz[1]), cv2.INTER_CUBIC)\n    elif np.argmax(voxel.shape) == 1:\n        for i, s in enumerate(np.linspace(0, voxel.shape[1] - 1, num=sz[1])):\n            sampled = voxel[:, int(s), :]\n            output[:, i, :] = cv2.resize(sampled, (sz[2], sz[0]), cv2.INTER_CUBIC)\n    elif np.argmax(voxel.shape) == 2:\n        for i, s in enumerate(np.linspace(0, voxel.shape[2] - 1, num=sz[2])):\n            sampled = voxel[:, :, int(s)]\n            output[:, :, i] = cv2.resize(sampled, (sz[1], sz[0]), cv2.INTER_CUBIC)\n    return output\n\ndef clahe_3d(voxel):\n    voxel = mclahe(voxel, kernel_size=[8,32,32],\n              n_bins=128,\n              clip_limit=0.01,\n              adaptive_hist_range=False)\n    return (voxel*255.).astype(np.uint8).clip(0, 255)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:44:30.197203Z","iopub.execute_input":"2021-09-21T18:44:30.197668Z","iopub.status.idle":"2021-09-21T18:44:30.209515Z","shell.execute_reply.started":"2021-09-21T18:44:30.197623Z","shell.execute_reply":"2021-09-21T18:44:30.208799Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def preprocess_data(split, study_id, scan_type) :\n    \n    '''\n    Combine all the data preprocessing functions\n    '''\n    \n    voxel = get_voxel(split, study_id, scan_type)\n    voxel = normalize_contrast(voxel)\n    voxel = crop_voxel(voxel)\n    voxel = filter_voxel(voxel, filter_thr = 0.4)\n    voxel = resize_voxel(voxel, sz=(64, 256, 256))\n    voxel = clahe_3d(voxel)\n    \n    return voxel","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:44:31.687116Z","iopub.execute_input":"2021-09-21T18:44:31.687433Z","iopub.status.idle":"2021-09-21T18:44:31.692525Z","shell.execute_reply.started":"2021-09-21T18:44:31.687401Z","shell.execute_reply":"2021-09-21T18:44:31.691476Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def create_final_tensor(split, study_id) :\n    \n    '''\n    Add Scan type as channels\n    '''\n    \n    voxel_list = []\n    \n    for scan in scan_types :\n        voxel = preprocess_data(split, study_id, scan)\n        voxel = np.expand_dims(voxel, 0)\n        voxel_list.append(voxel)\n        \n    return np.concatenate(voxel_list, axis =0)","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:48:59.025580Z","iopub.execute_input":"2021-09-21T18:48:59.026187Z","iopub.status.idle":"2021-09-21T18:48:59.031450Z","shell.execute_reply.started":"2021-09-21T18:48:59.026137Z","shell.execute_reply":"2021-09-21T18:48:59.030685Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class MRIScanDataset(Dataset) :\n    \n    def __init__(self, paths, targets=None,\n                 label_smoothing=0.01, split=\"train\", augment=False) :\n        \n        self.paths = paths\n        self.targets = targets\n        self.label_smoothing = label_smoothing\n        self.split = split\n        self.augment = augment\n        \n        \n    def __len__(self) :\n        return len(self.paths)\n    \n    def __getitem__(self, index) :\n        \n        study_id = self.paths[index]\n        \n        voxel = create_final_tensor(self.split, study_id)\n        gc.collect()\n            \n        if self.targets is None:\n            return {\"X\": torch.tensor(voxel).float(), \"id\": study_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(voxel).float(), \"y\": y}\n            ","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:49:46.895862Z","iopub.execute_input":"2021-09-21T18:49:46.896153Z","iopub.status.idle":"2021-09-21T18:49:46.904405Z","shell.execute_reply.started":"2021-09-21T18:49:46.896124Z","shell.execute_reply":"2021-09-21T18:49:46.903300Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        self.conv_layer1 = self._conv_layer_set(4, 16)\n        self.conv_layer2 = self._conv_layer_set(16, 32)\n        self.conv_layer3 = self._conv_layer_set(32, 64)\n        self.fc1 = nn.Linear(6*30*30*64, 64) \n        self.fc2 = nn.Linear(64, 1)\n        self.relu = nn.LeakyReLU()\n        self.batch=nn.BatchNorm1d(64)\n        self.drop=nn.Dropout(p=0.15)        \n        \n    def _conv_layer_set(self, in_c, out_c):\n        conv_layer = nn.Sequential(\n        nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),\n        nn.BatchNorm3d(out_c),\n        nn.LeakyReLU(),\n        nn.MaxPool3d((2, 2, 2)),\n        nn.Dropout3d(0.2)\n        )\n        return conv_layer\n    \n\n    def forward(self, x):\n        # Set 1\n#         print(x.shape)\n        out = self.conv_layer1(x)\n        out = self.conv_layer2(out)\n        out = self.conv_layer3(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.batch(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:50:07.199965Z","iopub.execute_input":"2021-09-21T18:50:07.201002Z","iopub.status.idle":"2021-09-21T18:50:07.212245Z","shell.execute_reply.started":"2021-09-21T18:50:07.200943Z","shell.execute_reply":"2021-09-21T18:50:07.211171Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class TrainModel:\n    \n    def __init__(self, model, device, \n                optimizer, criterion ):\n        \n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n        \n        self.best_valid_score = np.inf\n        self.n_patience = 0\n        self.lastmodel = None\n        \n    def fit(self, epochs, train_dataloader, valid_dataloader, save_path, patience):\n        \n        start_time = time.time()\n        \n        for epoch in range(1, epochs + 1):\n            \n            print(f'Running Epoch {epoch}...................')\n            gc.collect()\n            \n            train_loss, train_auc = self.train(train_dataloader)\n            val_loss, val_auc = self.validation(valid_dataloader)\n            \n            print(f'For Epoch {epoch :>7d} Train Loss {train_loss : >5f} Train AUC {train_auc} Val Loss {val_loss} Val AUC {val_auc} ')\n            print(f'For Epoch {epoch :>7d} Time Taken {(time.time() - start_time)/60}')\n            \n            \n            if self.best_valid_score > val_loss: \n                \n                self.save_model(epoch, save_path, val_loss, val_auc)\n                print(f'AUC Improved from {self.best_valid_score :4f} to {val_loss}. Saved model to {self.lastmodel}')\n                \n                #updating the lossed\n                self.best_valid_score = val_loss\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                print(f\"\\nValid auc didn't improve last {patience} epochs.\")\n                break\n            \n            \n            \n    def train(self, train_dataloader) :\n        '''\n        For Training the model.\n        We will be calculating batch wise loss and \n        finally calcualting auc on the overall y\n        '''\n    \n        self.model.train()\n        sum_loss = 0\n        y_all = []\n        output_all = []\n        start_time = time.time()\n\n        for batch, data in enumerate(train_dataloader) :\n\n            X = data['X'].to(self.device)\n            y = data['y'].to(self.device)\n            gc.collect()\n\n            self.optimizer.zero_grad()     #clearning the accumulated gradients\n            pred = self.model(X).squeeze(1)           #make the prediction\n            loss = self.criterion(pred, y) #calcualte the loss\n            loss.backward()           #backpropagation\n            self.optimizer.step()          #update weights\n\n            sum_loss += loss.detach().item()\n            y_all.extend(data['y'].tolist()) #save all y values to y_val\n            output_all.extend(torch.sigmoid(pred).tolist())  #save all pred to output all\n\n            #print peformance\n            if batch % 20 == 0 :\n                time_taken = (time.time() - start_time)\n                start_time = time.time()\n                print(f'Train Batch {batch + 1 :>7d} Loss : {sum_loss/(batch +1)} Time Taken : {time_taken/60} ')\n\n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        train_auc = roc_auc_score(y_all, output_all)\n\n        return sum_loss/len(train_dataloader) , train_auc\n    \n    \n    \n    def validation(self, val_dataloader) :\n    \n        self.model.eval()\n        sum_loss = 0\n        y_all = []\n        output_all = []\n\n        for batch, data in enumerate(val_dataloader) :\n\n            with torch.no_grad() :\n\n                X_val = data['X'].to(self.device)\n                y_val = data['y'].to(self.device)\n\n                pred = self.model(X_val).squeeze(1)   #make the prediction\n                loss = self.criterion(pred, y_val) #calcualte the loss\n\n                sum_loss += loss.detach().item()\n                y_all.extend(data['y'].tolist()) #save all y values to y_val\n                output_all.extend(torch.sigmoid(pred).tolist())  #save all pred to output all\n\n                #print peformance\n                if batch % 20 == 0 :\n                    print(f'Test Batch {batch + 1 :>7d} Loss : {sum_loss/(batch +1)}')\n\n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        val_auc = roc_auc_score(y_all, output_all)\n\n        return sum_loss/len(val_dataloader) , val_auc\n\n            \n    def save_model(self, n_epoch, save_path, loss, auc):\n        \n        self.lastmodel = f\"{save_path}_loss{loss:.3f}_auc{auc:.3f}.pth\"\n        torch.save(self.model.state_dict(), self.lastmodel)\n       ","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:50:31.414360Z","iopub.execute_input":"2021-09-21T18:50:31.414621Z","iopub.status.idle":"2021-09-21T18:50:31.433784Z","shell.execute_reply.started":"2021-09-21T18:50:31.414595Z","shell.execute_reply":"2021-09-21T18:50:31.433179Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#train the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_mri_type(df_train, df_valid):\n    \n    #train dataset\n    train_dataset = MRIScanDataset(\n        df_train[\"BraTS21ID\"].values, \n        df_train[\"MGMT_value\"].values, \n        augment=False)\n    \n    #valid dataset\n    valid_dataset = MRIScanDataset(\n        df_valid[\"BraTS21ID\"].values, \n        df_valid[\"MGMT_value\"].values)\n    \n    #train dataloader\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=8,\n        shuffle=True,\n    drop_last=True)\n\n    valid_loader = DataLoader(\n        valid_dataset, \n        batch_size=8,\n        shuffle=False,\n    drop_last=True)\n    \n    #load model\n    model = CNNModel()\n    model.to(device)\n    \n    #define optimizer & criterion\n    criterion = F.binary_cross_entropy_with_logits\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n    \n\n    trainer = TrainModel(model, device, \n                    optimizer,criterion)\n\n    history = trainer.fit(epochs= 10, train_dataloader= train_loader, valid_dataloader= valid_loader, save_path= 'model', patience= 6)\n    \n    return trainer.lastmodel","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:50:46.370702Z","iopub.execute_input":"2021-09-21T18:50:46.371314Z","iopub.status.idle":"2021-09-21T18:50:46.379794Z","shell.execute_reply.started":"2021-09-21T18:50:46.371277Z","shell.execute_reply":"2021-09-21T18:50:46.379184Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"try :\n    model = train_mri_type(df_train, df_valid)\n        \nexcept :\n    print('Error while handling model files')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(modelfile, df, split):\n    \n    data_retriever = MRIScanDataset(\n        df.index.values, \n        split=split\n    )\n\n    data_loader = DataLoader(\n        data_retriever,\n        batch_size=8,\n        shuffle=False\n    )\n   \n    model = CNNModel()\n    model.to(device)\n    \n    model.load_state_dict(torch.load(modelfile))\n    model.eval()\n    \n    y_pred = []\n    ids = []\n\n    for e, batch in enumerate(data_loader,1):\n        \n        print(f\"{e}/{len(data_loader)}\", end=\"\\r\")\n        gc.collect()\n        with torch.no_grad():\n            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            if tmp_pred.size == 1:\n                y_pred.append(tmp_pred)\n            else:\n                y_pred.extend(tmp_pred.tolist())\n            batch_id_list = list(batch[\"id\"])\n            ids.extend(batch_id_list)\n            \n    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred}) \n    preddf = preddf.set_index(\"BraTS21ID\")\n    return preddf","metadata":{"execution":{"iopub.status.busy":"2021-09-21T18:51:16.345824Z","iopub.execute_input":"2021-09-21T18:51:16.346273Z","iopub.status.idle":"2021-09-21T18:51:16.354278Z","shell.execute_reply.started":"2021-09-21T18:51:16.346241Z","shell.execute_reply":"2021-09-21T18:51:16.353343Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"try :\n    submission = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/sample_submission.csv')\n    submission = submission.set_index(\"BraTS21ID\")\n    pred = predict(model, submission, \"test\")\n    submission[f\"MGMT_value\"] = pred\n    submission['MGMT_value'].to_csv('submission.csv')\n    \nexcept :\n    print('Error while Submission')","metadata":{},"execution_count":null,"outputs":[]}]}