{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n#         print(os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-07T15:36:36.346364Z","iopub.execute_input":"2021-09-07T15:36:36.346809Z","iopub.status.idle":"2021-09-07T15:38:38.865735Z","shell.execute_reply.started":"2021-09-07T15:36:36.346715Z","shell.execute_reply":"2021-09-07T15:38:38.864443Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Support Notebook :\n\n1. https://www.kaggle.com/rluethy/efficientnet3d-with-one-mri-type\n2. https://github.com/shijianjian/EfficientNet-PyTorch-3D\n3. https://www.kaggle.com/davidbroberts/determining-mr-slice-orientation\n4. ","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/shijianjian/EfficientNet-PyTorch-3D","metadata":{"execution":{"iopub.status.busy":"2021-09-07T06:37:55.492515Z","iopub.status.idle":"2021-09-07T06:37:55.493364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys \nimport glob\nimport time\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from efficientnet_pytorch_3d import EfficientNet3D\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:09.723898Z","iopub.execute_input":"2021-09-07T15:41:09.724270Z","iopub.status.idle":"2021-09-07T15:41:12.778300Z","shell.execute_reply.started":"2021-09-07T15:41:09.724228Z","shell.execute_reply":"2021-09-07T15:41:12.777505Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_directory = '../input/rsna-miccai-brain-tumor-radiogenomic-classification'\n\nmri_types = ['FLAIR','T1w','T1wCE','T2w']\nSIZE = 128 #all the image sizes are of dim 256 x 256\nNUM_IMAGES = 64","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:12.779652Z","iopub.execute_input":"2021-09-07T15:41:12.780130Z","iopub.status.idle":"2021-09-07T15:41:12.784261Z","shell.execute_reply.started":"2021-09-07T15:41:12.780086Z","shell.execute_reply":"2021-09-07T15:41:12.783555Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 1. EDA ","metadata":{}},{"cell_type":"code","source":"#number of train & test samples\nprint(f\"Number of Sample in Train Data : {len(glob.glob(f'{data_directory}/train/*'))}\")\nprint(f\"Number of Sample in Test Data : {len(glob.glob(f'{data_directory}/test/*'))}\")","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:14.386453Z","iopub.execute_input":"2021-09-07T15:41:14.386987Z","iopub.status.idle":"2021-09-07T15:41:14.401755Z","shell.execute_reply.started":"2021-09-07T15:41:14.386955Z","shell.execute_reply":"2021-09-07T15:41:14.400673Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of Sample in Train Data : 585\nNumber of Sample in Test Data : 87\n","output_type":"stream"}]},{"cell_type":"code","source":"train_labels = pd.read_csv('../input/rsna-miccai-brain-tumor-radiogenomic-classification/train_labels.csv')\ntrain_labels.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T18:29:22.212398Z","iopub.execute_input":"2021-09-07T18:29:22.212993Z","iopub.status.idle":"2021-09-07T18:29:22.238643Z","shell.execute_reply.started":"2021-09-07T18:29:22.212957Z","shell.execute_reply":"2021-09-07T18:29:22.237497Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"   BraTS21ID  MGMT_value\n0          0           1\n1          2           1\n2          3           0\n3          5           1\n4          6           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BraTS21ID</th>\n      <th>MGMT_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels['MGMT_value'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:16.314791Z","iopub.execute_input":"2021-09-07T15:41:16.315192Z","iopub.status.idle":"2021-09-07T15:41:16.328004Z","shell.execute_reply.started":"2021-09-07T15:41:16.315157Z","shell.execute_reply":"2021-09-07T15:41:16.326740Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1    307\n0    278\nName: MGMT_value, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"## 1.1 Understanding a single SCAN file","metadata":{}},{"cell_type":"code","source":"def center_crop(img, dim):\n    'Center crop'\n    \n    width, height = img.shape[1], img.shape[0]\n    #process crop width and height for max available dimension\n    crop_width = dim[0] if dim[0]<img.shape[1] else img.shape[1]\n    crop_height = dim[1] if dim[1]<img.shape[0] else img.shape[0]\n    mid_x, mid_y = int(width/2), int(height/2)\n    cw2, ch2 = int(crop_width/2), int(crop_height/2) \n    crop_img = img[mid_y-ch2:mid_y+ch2, mid_x-cw2:mid_x+cw2]\n    return crop_img\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:16.657255Z","iopub.execute_input":"2021-09-07T15:41:16.657631Z","iopub.status.idle":"2021-09-07T15:41:16.664614Z","shell.execute_reply.started":"2021-09-07T15:41:16.657583Z","shell.execute_reply":"2021-09-07T15:41:16.663810Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#load dicom image\n#image size will be 128x128\ndef load_dicom_image(file_path, img_size = SIZE, voi_lut = True) :\n    \n    dicom = pydicom.read_file(file_path)\n    data = dicom.pixel_array\n    if voi_lut :\n        data = apply_voi_lut(data, dicom)\n        \n    data = center_crop(data, (128,128))\n    return data","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:16.773710Z","iopub.execute_input":"2021-09-07T15:41:16.774265Z","iopub.status.idle":"2021-09-07T15:41:16.780062Z","shell.execute_reply.started":"2021-09-07T15:41:16.774218Z","shell.execute_reply":"2021-09-07T15:41:16.778910Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#load a single image\nsplit = 'train'\nscan_id = '00002'\nmri_type = 'FLAIR'\n#glob will store files present in the provided path as per the pattern\nfiles = sorted(glob.glob(f\"{data_directory}/train/00002/FLAIR/*.dcm\"), \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\nprint(f'Number of dcm files : {len(files)}')\nprint(files[:5])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:16.955888Z","iopub.execute_input":"2021-09-07T15:41:16.956418Z","iopub.status.idle":"2021-09-07T15:41:16.977217Z","shell.execute_reply.started":"2021-09-07T15:41:16.956370Z","shell.execute_reply":"2021-09-07T15:41:16.975925Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Number of dcm files : 129\n['../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00002/FLAIR/Image-387.dcm', '../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00002/FLAIR/Image-388.dcm', '../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00002/FLAIR/Image-389.dcm', '../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00002/FLAIR/Image-390.dcm', '../input/rsna-miccai-brain-tumor-radiogenomic-classification/train/00002/FLAIR/Image-391.dcm']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The above scan file contains 129 dcm files for the FLAIR MRI type. Different scan files can contains different number of scan files. Below, we will try to plot the distribution of the number of files for each type of MRI","metadata":{}},{"cell_type":"code","source":"#FOR FLAIR\nscan_id_list = [str(x).rjust(5,'0') for x in train_labels['BraTS21ID'].tolist()]\nnum_of_dcm_files = [len(glob.glob(f\"{data_directory}/train/{scan_id}/FLAIR/*.dcm\")) for scan_id in scan_id_list]\nnum_of_dcm_files = np.array(num_of_dcm_files)\n\n#print percentiles\nprint(f'25th Percentile : {np.percentile(num_of_dcm_files, 25)}')\nprint(f'50th Percentile : {np.percentile(num_of_dcm_files, 50)}')\nprint(f'75th Percentile : {np.percentile(num_of_dcm_files, 75)}')\nplt.hist(num_of_dcm_files)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:19.412532Z","iopub.execute_input":"2021-09-07T15:41:19.412923Z","iopub.status.idle":"2021-09-07T15:41:20.731656Z","shell.execute_reply.started":"2021-09-07T15:41:19.412893Z","shell.execute_reply":"2021-09-07T15:41:20.730653Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"25th Percentile : 60.0\n50th Percentile : 60.0\n75th Percentile : 200.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#FOR T1W\nscan_id_list = [str(x).rjust(5,'0') for x in train_labels['BraTS21ID'].tolist()]\nnum_of_dcm_files = [len(glob.glob(f\"{data_directory}/train/{scan_id}/T1w/*.dcm\")) for scan_id in scan_id_list]\nnum_of_dcm_files = np.array(num_of_dcm_files)\n\n#print percentiles\nprint(f'25th Percentile : {np.percentile(num_of_dcm_files, 25)}')\nprint(f'50th Percentile : {np.percentile(num_of_dcm_files, 50)}')\nprint(f'75th Percentile : {np.percentile(num_of_dcm_files, 75)}')\nplt.hist(num_of_dcm_files)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:20.733430Z","iopub.execute_input":"2021-09-07T15:41:20.733765Z","iopub.status.idle":"2021-09-07T15:41:21.678939Z","shell.execute_reply.started":"2021-09-07T15:41:20.733733Z","shell.execute_reply":"2021-09-07T15:41:21.677818Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"25th Percentile : 32.0\n50th Percentile : 180.0\n75th Percentile : 192.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#FOR T1wCE\nscan_id_list = [str(x).rjust(5,'0') for x in train_labels['BraTS21ID'].tolist()]\nnum_of_dcm_files = [len(glob.glob(f\"{data_directory}/train/{scan_id}/T1wCE/*.dcm\")) for scan_id in scan_id_list]\nnum_of_dcm_files = np.array(num_of_dcm_files)\n\n#print percentiles\nprint(f'25th Percentile : {np.percentile(num_of_dcm_files, 25)}')\nprint(f'50th Percentile : {np.percentile(num_of_dcm_files, 50)}')\nprint(f'75th Percentile : {np.percentile(num_of_dcm_files, 75)}')\nplt.hist(num_of_dcm_files)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:21.681060Z","iopub.execute_input":"2021-09-07T15:41:21.681473Z","iopub.status.idle":"2021-09-07T15:41:22.736137Z","shell.execute_reply.started":"2021-09-07T15:41:21.681427Z","shell.execute_reply":"2021-09-07T15:41:22.735030Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"25th Percentile : 129.0\n50th Percentile : 192.0\n75th Percentile : 192.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#FOR T2w\nscan_id_list = [str(x).rjust(5,'0') for x in train_labels['BraTS21ID'].tolist()]\nnum_of_dcm_files = [len(glob.glob(f\"{data_directory}/train/{scan_id}/T2w/*.dcm\")) for scan_id in scan_id_list]\nnum_of_dcm_files = np.array(num_of_dcm_files)\n\n#print percentiles\nprint(f'25th Percentile : {np.percentile(num_of_dcm_files, 25)}')\nprint(f'50th Percentile : {np.percentile(num_of_dcm_files, 50)}')\nprint(f'75th Percentile : {np.percentile(num_of_dcm_files, 75)}')\nplt.hist(num_of_dcm_files)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:22.737501Z","iopub.execute_input":"2021-09-07T15:41:22.737817Z","iopub.status.idle":"2021-09-07T15:41:23.739536Z","shell.execute_reply.started":"2021-09-07T15:41:22.737788Z","shell.execute_reply":"2021-09-07T15:41:23.738292Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"25th Percentile : 64.0\n50th Percentile : 64.0\n75th Percentile : 376.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"'''\nHere, we are taking middle 64 images from the dicom files. The idea is to capture the middle section of brain and \nmaking the channels constant for all the files\n'''\nmiddle = len(files)//2\nnum_imgs2 = 64//2\np1 = max(0, middle - num_imgs2)\np2 = min(len(files), middle + num_imgs2)\nimg3d = np.stack([load_dicom_image(f) for f in files[p1:p2]]).T \nprint(img3d.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:23.741090Z","iopub.execute_input":"2021-09-07T15:41:23.741401Z","iopub.status.idle":"2021-09-07T15:41:24.888253Z","shell.execute_reply.started":"2021-09-07T15:41:23.741372Z","shell.execute_reply":"2021-09-07T15:41:24.887368Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"(128, 128, 64)\n","output_type":"stream"}]},{"cell_type":"code","source":"#padding it with zero dim if channel dimension is less than 64\nif img3d.shape[-1] < 64:\n    n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n    img3d = np.concatenate((img3d,  n_zero), axis = -1)\n        \n#min max scaling\nif np.min(img3d) < np.max(img3d):\n    img3d = img3d - np.min(img3d)\n    img3d = img3d / np.max(img3d)\n \n  \nimg3d = np.expand_dims(img3d,0) #to add the batch dimension\nprint(img3d.shape)\nimg3d[:,:,1]      ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:24.889423Z","iopub.execute_input":"2021-09-07T15:41:24.889934Z","iopub.status.idle":"2021-09-07T15:41:24.914593Z","shell.execute_reply.started":"2021-09-07T15:41:24.889900Z","shell.execute_reply":"2021-09-07T15:41:24.913297Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(1, 128, 128, 64)\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"array([[[0.        , 0.        , 0.        , ..., 0.        ,\n         0.        , 0.        ],\n        [0.        , 0.        , 0.        , ..., 0.        ,\n         0.        , 0.        ],\n        [0.        , 0.        , 0.        , ..., 0.        ,\n         0.        , 0.        ],\n        ...,\n        [0.43211369, 0.52489683, 0.49331141, ..., 0.29574923,\n         0.16097188, 0.        ],\n        [0.43407963, 0.52131527, 0.48755739, ..., 0.27772573,\n         0.14918354, 0.        ],\n        [0.43250688, 0.51455011, 0.48679019, ..., 0.27076211,\n         0.09186779, 0.        ]]])"},"metadata":{}}]},{"cell_type":"code","source":"'''\nPutting everything in a single function\n'''\ndef load_dicom_images_3d(scan_id, num_imgs=NUM_IMAGES, \n                         img_size=SIZE, mri_type=\"FLAIR\", \n                         split=\"train\"):\n\n    files = sorted(glob.glob(f\"{data_directory}/{split}/{scan_id}/{mri_type}/*.dcm\"), \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n\n    middle = len(files)//2\n    num_imgs2 = num_imgs//2\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(files), middle + num_imgs2)\n    img3d = np.stack([load_dicom_image(f) for f in files[p1:p2]]).T \n    if img3d.shape[-1] < num_imgs:\n        n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n        img3d = np.concatenate((img3d,  n_zero), axis = -1)\n        \n    if np.min(img3d) < np.max(img3d):\n        img3d = img3d - np.min(img3d)\n        img3d = img3d / np.max(img3d)\n        \n    img3d = np.expand_dims(img3d,0)\n    img3d = np.transpose(img3d, (0,3,1,2))\n            \n    return img3d\n\na = load_dicom_images_3d(\"00000\")\nprint(a.shape)\nprint(np.min(a), np.max(a), np.mean(a), np.median(a))","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:24.917000Z","iopub.execute_input":"2021-09-07T15:41:24.917456Z","iopub.status.idle":"2021-09-07T15:41:26.047699Z","shell.execute_reply.started":"2021-09-07T15:41:24.917420Z","shell.execute_reply":"2021-09-07T15:41:26.046539Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(1, 64, 128, 128)\n0.0 1.0 0.3412781428862095 0.3668791707614164\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train & Test Split","metadata":{}},{"cell_type":"code","source":"def set_seed(seed):\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(12)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:26.049295Z","iopub.execute_input":"2021-09-07T15:41:26.049591Z","iopub.status.idle":"2021-09-07T15:41:26.059466Z","shell.execute_reply.started":"2021-09-07T15:41:26.049562Z","shell.execute_reply":"2021-09-07T15:41:26.058014Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_train, df_valid = train_test_split(\n    train_labels, \n    test_size=0.2, \n    random_state=42, \n    stratify=train_labels[\"MGMT_value\"],\n)\n\nprint(df_train.shape, df_valid.shape)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T18:29:31.058949Z","iopub.execute_input":"2021-09-07T18:29:31.059312Z","iopub.status.idle":"2021-09-07T18:29:31.072104Z","shell.execute_reply.started":"2021-09-07T18:29:31.059281Z","shell.execute_reply":"2021-09-07T18:29:31.070816Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"(468, 2) (117, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model and Training Class","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:29.412838Z","iopub.execute_input":"2021-09-07T15:41:29.413325Z","iopub.status.idle":"2021-09-07T15:41:29.418592Z","shell.execute_reply.started":"2021-09-07T15:41:29.413293Z","shell.execute_reply":"2021-09-07T15:41:29.417634Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class MRIScanDataset(Dataset) :\n    \n    def __init__(self, paths, targets=None, mri_type=None, \n                 label_smoothing=0.01, split=\"train\", augment=False) :\n        \n        self.paths = paths\n        self.targets = targets\n        self.mri_type = mri_type\n        self.label_smoothing = label_smoothing\n        self.split = split\n        self.augment = augment\n        \n        \n    def __len__(self) :\n        return len(self.paths)\n    \n    def __getitem__(self, index) :\n        \n        scan_id = self.paths[index]\n        if self.targets is None:\n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index],\n                                        split=self.split)\n        else:\n            if self.augment:\n                rotation = np.random.randint(0,4)\n            else:\n                rotation = 0\n                \n            data = load_dicom_images_3d(str(scan_id).zfill(5), mri_type=self.mri_type[index], \n                                        split=\"train\")\n            \n            \n        if self.targets is None:\n            return {\"X\": torch.tensor(data).float(), \"id\": scan_id}\n        else:\n            y = torch.tensor(abs(self.targets[index]-self.label_smoothing), dtype=torch.float)\n            return {\"X\": torch.tensor(data).float(), \"y\": y}\n               ","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:31.214859Z","iopub.execute_input":"2021-09-07T15:41:31.215523Z","iopub.status.idle":"2021-09-07T15:41:31.231058Z","shell.execute_reply.started":"2021-09-07T15:41:31.215477Z","shell.execute_reply":"2021-09-07T15:41:31.229751Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# class Model(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.net = EfficientNet3D.from_name(\"efficientnet-b0\", override_params={'num_classes': 2}, in_channels=1)\n#         n_features = self.net._fc.in_features\n#         self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n    \n#     def forward(self, x):\n#         out = self.net(x)\n#         return out","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:31.443442Z","iopub.execute_input":"2021-09-07T15:41:31.443833Z","iopub.status.idle":"2021-09-07T15:41:31.449519Z","shell.execute_reply.started":"2021-09-07T15:41:31.443800Z","shell.execute_reply":"2021-09-07T15:41:31.448389Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        self.conv_layer1 = self._conv_layer_set(1, 16)\n        self.conv_layer2 = self._conv_layer_set(16, 32)\n        self.fc1 = nn.Linear(14*30*30*32, 64)\n        self.fc2 = nn.Linear(64, 1)\n        self.relu = nn.LeakyReLU()\n        self.batch=nn.BatchNorm1d(64)\n        self.drop=nn.Dropout(p=0.15)        \n        \n    def _conv_layer_set(self, in_c, out_c):\n        conv_layer = nn.Sequential(\n        nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),\n        nn.LeakyReLU(),\n        nn.MaxPool3d((2, 2, 2)),\n        )\n        return conv_layer\n    \n\n    def forward(self, x):\n        # Set 1\n        out = self.conv_layer1(x)\n        out = self.conv_layer2(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.batch(out)\n        out = self.drop(out)\n        out = self.fc2(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:41:31.632759Z","iopub.execute_input":"2021-09-07T15:41:31.633366Z","iopub.status.idle":"2021-09-07T15:41:31.645059Z","shell.execute_reply.started":"2021-09-07T15:41:31.633317Z","shell.execute_reply":"2021-09-07T15:41:31.643989Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class TrainModel:\n    \n    def __init__(self, model, device, \n                optimizer, criterion ):\n        \n        self.model = model\n        self.device = device\n        self.optimizer = optimizer\n        self.criterion = criterion\n        \n        self.best_valid_score = np.inf\n        self.n_patience = 0\n        self.lastmodel = None\n        \n    def fit(self, epochs, train_dataloader, valid_dataloader, save_path, patience):\n        \n        start_time = time.time()\n        \n        for epoch in range(1, epochs + 1):\n            \n            print(f'Running Epoch {epoch}...................')\n            \n            train_loss, train_auc = self.train(train_dataloader)\n            val_loss, val_auc = self.validation(valid_dataloader)\n            \n            print(f'For Epoch {epoch :>7d} Train Loss {train_loss : >5f} Train AUC {train_auc} Val Loss {val_loss} Val AUC {val_auc} ')\n            print(f'For Epoch {epoch :>7d} Time Taken {(time.time() - start_time)/60}')\n            \n            \n            if self.best_valid_score > val_loss: \n                \n                self.save_model(epoch, save_path, val_loss, val_auc)\n                print(f'AUC Improved from {self.best_valid_score :4f} to {val_loss}. Saved model to {self.lastmodel}')\n                \n                #updating the lossed\n                self.best_valid_score = val_loss\n                self.n_patience = 0\n            else:\n                self.n_patience += 1\n            \n            if self.n_patience >= patience:\n                print(f\"\\nValid auc didn't improve last {patience} epochs.\")\n                break\n            \n            \n            \n    def train(self, train_dataloader) :\n        '''\n        For Training the model.\n        We will be calculating batch wise loss and \n        finally calcualting auc on the overall y\n        '''\n    \n        self.model.train()\n        sum_loss = 0\n        y_all = []\n        output_all = []\n        start_time = time.time()\n\n        for batch, data in enumerate(train_dataloader) :\n\n            X = data['X'].to(self.device)\n            y = data['y'].to(self.device)\n\n            self.optimizer.zero_grad()     #clearning the accumulated gradients\n            pred = self.model(X).squeeze(1)           #make the prediction\n            loss = self.criterion(pred, y) #calcualte the loss\n            loss.backward()           #backpropagation\n            self.optimizer.step()          #update weights\n\n            sum_loss += loss.detach().item()\n            y_all.extend(data['y'].tolist()) #save all y values to y_val\n            output_all.extend(torch.sigmoid(pred).tolist())  #save all pred to output all\n\n            #print peformance\n            if batch % 5 == 0 :\n                time_taken = (time.time() - start_time)\n                start_time = time.time()\n                print(f'Train Batch {batch + 1 :>7d} Loss : {sum_loss/(batch +1)} Time Taken : {time_taken/60} ')\n\n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        train_auc = roc_auc_score(y_all, output_all)\n\n        return sum_loss/len(train_dataloader) , train_auc\n    \n    \n    \n    def validation(self, val_dataloader) :\n    \n        self.model.eval()\n        sum_loss = 0\n        y_all = []\n        output_all = []\n\n        for batch, data in enumerate(val_dataloader) :\n\n            with torch.no_grad() :\n\n                X_val = data['X'].to(self.device)\n                y_val = data['y'].to(self.device)\n\n                pred = self.model(X_val).squeeze(1)   #make the prediction\n                loss = self.criterion(pred, y_val) #calcualte the loss\n\n                sum_loss += loss.detach().item()\n                y_all.extend(data['y'].tolist()) #save all y values to y_val\n                output_all.extend(torch.sigmoid(pred).tolist())  #save all pred to output all\n\n                #print peformance\n                if batch % 5 == 0 :\n                    print(f'Test Batch {batch + 1 :>7d} Loss : {sum_loss/(batch +1)}')\n\n        y_all = [1 if x > 0.5 else 0 for x in y_all]\n        val_auc = roc_auc_score(y_all, output_all)\n\n        return sum_loss/len(val_dataloader) , val_auc\n\n            \n    def save_model(self, n_epoch, save_path, loss, auc):\n        \n        self.lastmodel = f\"{save_path}-e{n_epoch}-loss{loss:.3f}-auc{auc:.3f}.pth\"\n        torch.save(\n            {\n                \"model_state_dict\": self.model.state_dict(),\n                \"optimizer_state_dict\": self.optimizer.state_dict(),\n                \"best_valid_score\": self.best_valid_score,\n                \"n_epoch\": n_epoch,\n            },\n            self.lastmodel,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:42:26.950346Z","iopub.execute_input":"2021-09-07T15:42:26.950727Z","iopub.status.idle":"2021-09-07T15:42:26.975772Z","shell.execute_reply.started":"2021-09-07T15:42:26.950692Z","shell.execute_reply":"2021-09-07T15:42:26.974408Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# df_train.loc[:,\"MRI_Type\"] = 'FLAIR'\n# train_dataset = MRIScanDataset(\n#         df_train[\"BraTS21ID\"].values, \n#         df_train[\"MGMT_value\"].values, \n#         df_train[\"MRI_Type\"].values,\n#         augment=False)\n\n# train_loader = DataLoader(\n#         train_dataset,\n#         batch_size=16,\n#         shuffle=True,\n#         num_workers=8,pin_memory = True )\n\n\n# for data in train_loader :\n#     X1 = data['X']\n#     y1 = data['y']\n#     print(X1.shape)\n#     print(y1.shape)\n#     break","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:42:27.288534Z","iopub.execute_input":"2021-09-07T15:42:27.288973Z","iopub.status.idle":"2021-09-07T15:42:27.293648Z","shell.execute_reply.started":"2021-09-07T15:42:27.288937Z","shell.execute_reply":"2021-09-07T15:42:27.292554Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndef train_mri_type(df_train, df_valid, mri_type):\n    \n    #creating data based on selected mri_type\n    if mri_type==\"all\":\n        train_list = []\n        valid_list = []\n        for mri_type in mri_types:\n            df_train.loc[:,\"MRI_Type\"] = mri_type\n            train_list.append(df_train.copy())\n            df_valid.loc[:,\"MRI_Type\"] = mri_type\n            valid_list.append(df_valid.copy())\n\n        df_train = pd.concat(train_list)\n        df_valid = pd.concat(valid_list)\n        \n    else:\n        df_train.loc[:,\"MRI_Type\"] = mri_type\n        df_valid.loc[:,\"MRI_Type\"] = mri_type\n\n    print(df_train.shape, df_valid.shape)\n    display(df_train.head())\n    \n    #train dataset\n    train_dataset = MRIScanDataset(\n        df_train[\"BraTS21ID\"].values, \n        df_train[\"MGMT_value\"].values, \n        df_train[\"MRI_Type\"].values,\n        augment=False)\n    \n    #valid dataset\n    valid_dataset = MRIScanDataset(\n        df_valid[\"BraTS21ID\"].values, \n        df_valid[\"MGMT_value\"].values,\n        df_valid[\"MRI_Type\"].values)\n    \n    #train dataloader\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=4,\n        shuffle=True,\n        num_workers=8,pin_memory = True )\n\n    valid_loader = DataLoader(\n        valid_dataset, \n        batch_size=4,\n        shuffle=False,\n        num_workers=8,pin_memory = True\n    )\n    \n    #load model\n    model = CNNModel()\n    model.to(device)\n    \n    #define optimizer & criterion\n    criterion = F.binary_cross_entropy_with_logits\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    \n\n    trainer = TrainModel(model, device, \n                    optimizer,criterion)\n\n    history = trainer.fit(epochs= 5, train_dataloader= train_loader, valid_dataloader= valid_loader, save_path= f\"{mri_type}\", patience= 5)\n    \n    return trainer.lastmodel\n","metadata":{"execution":{"iopub.status.busy":"2021-09-07T19:28:34.475633Z","iopub.execute_input":"2021-09-07T19:28:34.476020Z","iopub.status.idle":"2021-09-07T19:28:34.492240Z","shell.execute_reply.started":"2021-09-07T19:28:34.475987Z","shell.execute_reply":"2021-09-07T19:28:34.491069Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"modelfiles = None\n\nif not modelfiles:\n    modelfiles = [train_mri_type(df_train, df_valid, m) for m in ['T2w']]\n    print(modelfiles)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T18:29:37.628434Z","iopub.execute_input":"2021-09-07T18:29:37.628856Z","iopub.status.idle":"2021-09-07T19:22:10.422548Z","shell.execute_reply.started":"2021-09-07T18:29:37.628821Z","shell.execute_reply":"2021-09-07T19:22:10.421389Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"(468, 3) (117, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     BraTS21ID  MGMT_value MRI_Type\n102        154           0      T2w\n161        240           1      T2w\n508        740           1      T2w\n495        725           1      T2w\n298        432           0      T2w","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BraTS21ID</th>\n      <th>MGMT_value</th>\n      <th>MRI_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>102</th>\n      <td>154</td>\n      <td>0</td>\n      <td>T2w</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>240</td>\n      <td>1</td>\n      <td>T2w</td>\n    </tr>\n    <tr>\n      <th>508</th>\n      <td>740</td>\n      <td>1</td>\n      <td>T2w</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>725</td>\n      <td>1</td>\n      <td>T2w</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>432</td>\n      <td>0</td>\n      <td>T2w</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Running Epoch 1...................\nTrain Batch       1 Loss : 0.6563823819160461 Time Taken : 0.18688934644063313 \nTrain Batch       6 Loss : 0.8759249349435171 Time Taken : 0.22840533256530762 \nTrain Batch      11 Loss : 0.8175291771238501 Time Taken : 0.23779894908269247 \nTrain Batch      16 Loss : 0.7811290267854929 Time Taken : 0.22878638903299967 \nTrain Batch      21 Loss : 0.7521119302227384 Time Taken : 0.23124816020329794 \nTrain Batch      26 Loss : 0.7959985584020615 Time Taken : 0.22467987140019735 \nTrain Batch      31 Loss : 0.77107187336491 Time Taken : 0.2261155366897583 \nTrain Batch      36 Loss : 0.7713348178399934 Time Taken : 0.2260476549466451 \nTrain Batch      41 Loss : 0.7739777281516935 Time Taken : 0.2232532540957133 \nTrain Batch      46 Loss : 0.7777410518863926 Time Taken : 0.22376630703608194 \nTrain Batch      51 Loss : 0.7653867783499699 Time Taken : 0.22585370937983196 \nTrain Batch      56 Loss : 0.7562201124216829 Time Taken : 0.23823639551798503 \nTrain Batch      61 Loss : 0.7517615678857584 Time Taken : 0.22402891715367634 \nTrain Batch      66 Loss : 0.7490995223775054 Time Taken : 0.22370715141296388 \nTrain Batch      71 Loss : 0.7482876849006599 Time Taken : 0.24099244276682535 \nTrain Batch      76 Loss : 0.7453236795569721 Time Taken : 0.22614384889602662 \nTrain Batch      81 Loss : 0.744302068595533 Time Taken : 0.2206718921661377 \nTrain Batch      86 Loss : 0.7427666973929072 Time Taken : 0.22976008256276448 \nTrain Batch      91 Loss : 0.7491758665540716 Time Taken : 0.22262961864471437 \nTrain Batch      96 Loss : 0.7485674715911349 Time Taken : 0.2301977237065633 \nTrain Batch     101 Loss : 0.7448523242284756 Time Taken : 0.22135421832402546 \nTrain Batch     106 Loss : 0.7448561503639761 Time Taken : 0.1949134866396586 \nTrain Batch     111 Loss : 0.7428959068414327 Time Taken : 0.1850108027458191 \nTrain Batch     116 Loss : 0.7430812112730125 Time Taken : 0.1880481282869975 \nTest Batch       1 Loss : 0.8098090887069702\nTest Batch       6 Loss : 0.7199245095252991\nTest Batch      11 Loss : 0.7330593520944769\nTest Batch      16 Loss : 0.7390223890542984\nTest Batch      21 Loss : 0.7622990523065839\nTest Batch      26 Loss : 0.7801127663025489\nFor Epoch       1 Train Loss 0.743534 Train AUC 0.49741814985717425 Val Loss 0.7710429469744364 Val AUC 0.4156908665105386 \nFor Epoch       1 Time Taken 6.062231985727946\nAUC Improved from  inf to 0.7710429469744364. Saved model to T2w-e1-loss0.771-auc0.416.pth\nRunning Epoch 2...................\nTrain Batch       1 Loss : 0.599149763584137 Time Taken : 0.16560384035110473 \nTrain Batch       6 Loss : 0.6216149926185608 Time Taken : 0.221924356619517 \nTrain Batch      11 Loss : 0.6804133707826788 Time Taken : 0.2268401821454366 \nTrain Batch      16 Loss : 0.6999163664877415 Time Taken : 0.21593573093414306 \nTrain Batch      21 Loss : 0.7120777453695025 Time Taken : 0.22053971687952678 \nTrain Batch      26 Loss : 0.6873245514356173 Time Taken : 0.21956735849380493 \nTrain Batch      31 Loss : 0.6943185098709599 Time Taken : 0.2221456487973531 \nTrain Batch      36 Loss : 0.702484698759185 Time Taken : 0.2213340361913045 \nTrain Batch      41 Loss : 0.7101447800310646 Time Taken : 0.22220974365870158 \nTrain Batch      46 Loss : 0.7079433617384537 Time Taken : 0.22291797002156574 \nTrain Batch      51 Loss : 0.7074958915803947 Time Taken : 0.21788689295450847 \nTrain Batch      56 Loss : 0.7042272346360343 Time Taken : 0.21646867593129476 \nTrain Batch      61 Loss : 0.6985985705109893 Time Taken : 0.2201632261276245 \nTrain Batch      66 Loss : 0.6997219351204959 Time Taken : 0.21643294095993043 \nTrain Batch      71 Loss : 0.6976868535431338 Time Taken : 0.22829281091690062 \nTrain Batch      76 Loss : 0.6987958393598858 Time Taken : 0.22499616146087648 \nTrain Batch      81 Loss : 0.6992223866191911 Time Taken : 0.2211358110109965 \nTrain Batch      86 Loss : 0.6941841334797615 Time Taken : 0.22676747639973957 \nTrain Batch      91 Loss : 0.6928295519325759 Time Taken : 0.22032081683476765 \nTrain Batch      96 Loss : 0.6909340576579174 Time Taken : 0.22542344331741332 \nTrain Batch     101 Loss : 0.6897112709460872 Time Taken : 0.22642561197280883 \nTrain Batch     106 Loss : 0.6854266926365079 Time Taken : 0.18254215717315675 \nTrain Batch     111 Loss : 0.6856265994342597 Time Taken : 0.19114680687586466 \nTrain Batch     116 Loss : 0.686678045525633 Time Taken : 0.18323859373728435 \nTest Batch       1 Loss : 0.6355684399604797\nTest Batch       6 Loss : 0.6300050169229507\nTest Batch      11 Loss : 0.679226509549401\nTest Batch      16 Loss : 0.6926762964576483\nTest Batch      21 Loss : 0.7274326384067535\nTest Batch      26 Loss : 0.7283297215516751\nFor Epoch       2 Train Loss 0.686065 Train AUC 0.5815388559290998 Val Loss 0.7256436457236608 Val AUC 0.4739461358313817 \nFor Epoch       2 Time Taken 11.914813975493113\nAUC Improved from 0.771043 to 0.7256436457236608. Saved model to T2w-e2-loss0.726-auc0.474.pth\nRunning Epoch 3...................\nTrain Batch       1 Loss : 0.7579478621482849 Time Taken : 0.16627642313639324 \nTrain Batch       6 Loss : 0.5807531575361887 Time Taken : 0.21907855272293092 \nTrain Batch      11 Loss : 0.6689082925969904 Time Taken : 0.21781889200210572 \nTrain Batch      16 Loss : 0.6361839324235916 Time Taken : 0.21715502738952636 \nTrain Batch      21 Loss : 0.6416366327376593 Time Taken : 0.2208969036738078 \nTrain Batch      26 Loss : 0.6507968100217673 Time Taken : 0.22996469338734946 \nTrain Batch      31 Loss : 0.660583621071231 Time Taken : 0.22899797757466633 \nTrain Batch      36 Loss : 0.6565449717972014 Time Taken : 0.22614264885584515 \nTrain Batch      41 Loss : 0.6526456104546059 Time Taken : 0.21797464688618978 \nTrain Batch      46 Loss : 0.6433647436940152 Time Taken : 0.21472912629445393 \nTrain Batch      51 Loss : 0.6536352663647895 Time Taken : 0.2161658008893331 \nTrain Batch      56 Loss : 0.6545606718531677 Time Taken : 0.21780585447947184 \nTrain Batch      61 Loss : 0.6594301427974075 Time Taken : 0.22056409517923992 \nTrain Batch      66 Loss : 0.6663916648337336 Time Taken : 0.22122968037923177 \nTrain Batch      71 Loss : 0.6663958753498507 Time Taken : 0.21485294898351034 \nTrain Batch      76 Loss : 0.6605901510307663 Time Taken : 0.23300991853078207 \nTrain Batch      81 Loss : 0.6603696445624033 Time Taken : 0.2221524755160014 \nTrain Batch      86 Loss : 0.6590755654628887 Time Taken : 0.22780516544977825 \nTrain Batch      91 Loss : 0.6618862528722365 Time Taken : 0.2348768989245097 \nTrain Batch      96 Loss : 0.6667299348240098 Time Taken : 0.21856674750645955 \nTrain Batch     101 Loss : 0.6684729589684175 Time Taken : 0.2214763561884562 \nTrain Batch     106 Loss : 0.6710151942833414 Time Taken : 0.18180357217788695 \nTrain Batch     111 Loss : 0.6710363097556002 Time Taken : 0.18068954944610596 \nTrain Batch     116 Loss : 0.6764872436379564 Time Taken : 0.1877461075782776 \nTest Batch       1 Loss : 0.6056804656982422\nTest Batch       6 Loss : 0.6249249577522278\nTest Batch      11 Loss : 0.652398017319766\nTest Batch      16 Loss : 0.6460253372788429\nTest Batch      21 Loss : 0.6683401380266462\nTest Batch      26 Loss : 0.6778503862711099\nFor Epoch       3 Train Loss 0.676579 Train AUC 0.6030725847799018 Val Loss 0.6737768908341726 Val AUC 0.6086065573770492 \nFor Epoch       3 Time Taken 17.76197927792867\nAUC Improved from 0.725644 to 0.6737768908341726. Saved model to T2w-e3-loss0.674-auc0.609.pth\nRunning Epoch 4...................\nTrain Batch       1 Loss : 0.6158196926116943 Time Taken : 0.15685843626658122 \nTrain Batch       6 Loss : 0.6535104016462961 Time Taken : 0.22452201843261718 \nTrain Batch      11 Loss : 0.6750967827710238 Time Taken : 0.21700620253880817 \nTrain Batch      16 Loss : 0.6690273433923721 Time Taken : 0.22407889366149902 \nTrain Batch      21 Loss : 0.6568545968759627 Time Taken : 0.22862412532170615 \nTrain Batch      26 Loss : 0.6579316132343732 Time Taken : 0.2214490254720052 \nTrain Batch      31 Loss : 0.6664967527312617 Time Taken : 0.22193425099054973 \nTrain Batch      36 Loss : 0.66643369032277 Time Taken : 0.22200579643249513 \nTrain Batch      41 Loss : 0.6593776563318764 Time Taken : 0.21683643658955892 \nTrain Batch      46 Loss : 0.657196137568225 Time Taken : 0.21740996440251667 \nTrain Batch      51 Loss : 0.6610676149527231 Time Taken : 0.2179740071296692 \nTrain Batch      56 Loss : 0.6511893863124507 Time Taken : 0.22193214495976765 \nTrain Batch      61 Loss : 0.6485289379221494 Time Taken : 0.22367459932963055 \nTrain Batch      66 Loss : 0.6585832378177932 Time Taken : 0.22522290150324503 \nTrain Batch      71 Loss : 0.663006947493889 Time Taken : 0.21591587861378989 \nTrain Batch      76 Loss : 0.6594905441528872 Time Taken : 0.22423384189605713 \nTrain Batch      81 Loss : 0.6604444476557366 Time Taken : 0.2256144603093465 \nTrain Batch      86 Loss : 0.6600441042073938 Time Taken : 0.21994752883911134 \nTrain Batch      91 Loss : 0.661932804099806 Time Taken : 0.21936793327331544 \nTrain Batch      96 Loss : 0.6655580323810378 Time Taken : 0.21830988725026448 \nTrain Batch     101 Loss : 0.6620125820731172 Time Taken : 0.22644576629002888 \nTrain Batch     106 Loss : 0.6626965524453037 Time Taken : 0.17880012194315592 \nTrain Batch     111 Loss : 0.6634442819668366 Time Taken : 0.18218854665756226 \nTrain Batch     116 Loss : 0.6632561891757208 Time Taken : 0.18668513695398967 \nTest Batch       1 Loss : 0.524009108543396\nTest Batch       6 Loss : 0.593023955821991\nTest Batch      11 Loss : 0.6151804815639149\nTest Batch      16 Loss : 0.6022290028631687\nTest Batch      21 Loss : 0.6214220566408974\nTest Batch      26 Loss : 0.62751982991512\nFor Epoch       4 Train Loss 0.665979 Train AUC 0.6284699333479822 Val Loss 0.6438693523406982 Val AUC 0.6964285714285714 \nFor Epoch       4 Time Taken 23.58443707227707\nAUC Improved from 0.673777 to 0.6438693523406982. Saved model to T2w-e4-loss0.644-auc0.696.pth\nRunning Epoch 5...................\nTrain Batch       1 Loss : 0.7891934514045715 Time Taken : 0.1530347188313802 \nTrain Batch       6 Loss : 0.6767716507116953 Time Taken : 0.2205769697825114 \nTrain Batch      11 Loss : 0.6350777826525948 Time Taken : 0.23441698551177978 \nTrain Batch      16 Loss : 0.633730748668313 Time Taken : 0.22489479382832844 \nTrain Batch      21 Loss : 0.6338292970543816 Time Taken : 0.2262679974238078 \nTrain Batch      26 Loss : 0.6243848514098388 Time Taken : 0.22487459182739258 \nTrain Batch      31 Loss : 0.6268657282475503 Time Taken : 0.22350944677988688 \nTrain Batch      36 Loss : 0.6331845546762148 Time Taken : 0.2284414291381836 \nTrain Batch      41 Loss : 0.631727818308807 Time Taken : 0.22011607885360718 \nTrain Batch      46 Loss : 0.6370407466007315 Time Taken : 0.22589282989501952 \nTrain Batch      51 Loss : 0.6459105406321731 Time Taken : 0.2257663607597351 \nTrain Batch      56 Loss : 0.6467098386159965 Time Taken : 0.22438740730285645 \nTrain Batch      61 Loss : 0.652505023069069 Time Taken : 0.22829924821853637 \nTrain Batch      66 Loss : 0.6498831316377177 Time Taken : 0.22399011452992756 \nTrain Batch      71 Loss : 0.6509717984098784 Time Taken : 0.2271396279335022 \nTrain Batch      76 Loss : 0.6533322663683641 Time Taken : 0.2172439734141032 \nTrain Batch      81 Loss : 0.6528734064396516 Time Taken : 0.22338350216547648 \nTrain Batch      86 Loss : 0.6558232009410858 Time Taken : 0.22037015755971273 \nTrain Batch      91 Loss : 0.6577288442915612 Time Taken : 0.22249807516733805 \nTrain Batch      96 Loss : 0.6524324584752321 Time Taken : 0.23319693406422934 \nTrain Batch     101 Loss : 0.6542605462640819 Time Taken : 0.22489697535832723 \nTrain Batch     106 Loss : 0.6507509671292215 Time Taken : 0.1873335321744283 \nTrain Batch     111 Loss : 0.6511683829195865 Time Taken : 0.18337201674779255 \nTrain Batch     116 Loss : 0.6476706533596434 Time Taken : 0.18268555402755737 \nTest Batch       1 Loss : 0.5106326341629028\nTest Batch       6 Loss : 0.5887713233629862\nTest Batch      11 Loss : 0.6447905789722096\nTest Batch      16 Loss : 0.6520556546747684\nTest Batch      21 Loss : 0.6614701407296317\nTest Batch      26 Loss : 0.6540994002268865\nFor Epoch       5 Train Loss 0.647837 Train AUC 0.6774518420859884 Val Loss 0.6778026054302851 Val AUC 0.6121194379391101 \nFor Epoch       5 Time Taken 29.48417172431946\nRunning Epoch 6...................\nTrain Batch       1 Loss : 0.7312837839126587 Time Taken : 0.15756625731786092 \nTrain Batch       6 Loss : 0.5551220079263052 Time Taken : 0.2151374061902364 \nTrain Batch      11 Loss : 0.5358582843433727 Time Taken : 0.21978796323140462 \nTrain Batch      16 Loss : 0.5936663597822189 Time Taken : 0.2182172417640686 \nTrain Batch      21 Loss : 0.5946788475626991 Time Taken : 0.2271515409151713 \nTrain Batch      26 Loss : 0.5971463483113509 Time Taken : 0.2168003519376119 \nTrain Batch      31 Loss : 0.5983427032347648 Time Taken : 0.2226322929064433 \nTrain Batch      36 Loss : 0.5958096053865221 Time Taken : 0.22632642189661661 \nTrain Batch      41 Loss : 0.5974329470134363 Time Taken : 0.22109049161275227 \nTrain Batch      46 Loss : 0.5898616812799288 Time Taken : 0.22436219453811646 \nTrain Batch      51 Loss : 0.6046696568236631 Time Taken : 0.22484962940216063 \nTrain Batch      56 Loss : 0.6059795900114945 Time Taken : 0.22394774754842123 \nTrain Batch      61 Loss : 0.6052929314433552 Time Taken : 0.2236081838607788 \nTrain Batch      66 Loss : 0.6047976224711447 Time Taken : 0.22886117696762084 \nTrain Batch      71 Loss : 0.6096726269789146 Time Taken : 0.2253345529238383 \nTrain Batch      76 Loss : 0.6071504370162362 Time Taken : 0.21877750158309936 \nTrain Batch      81 Loss : 0.6027135212480286 Time Taken : 0.21991779804229736 \nTrain Batch      86 Loss : 0.6061138964669649 Time Taken : 0.22237715323766072 \nTrain Batch      91 Loss : 0.6174817343989571 Time Taken : 0.22396324078241983 \nTrain Batch      96 Loss : 0.625143786581854 Time Taken : 0.2209629535675049 \nTrain Batch     101 Loss : 0.6229441106909572 Time Taken : 0.21876932779947916 \nTrain Batch     106 Loss : 0.628977710346006 Time Taken : 0.18445914189020793 \nTrain Batch     111 Loss : 0.6330822529019536 Time Taken : 0.18216110467910768 \nTrain Batch     116 Loss : 0.6345257723125918 Time Taken : 0.18557650248209637 \nTest Batch       1 Loss : 0.4130629301071167\nTest Batch       6 Loss : 0.6042680541674296\nTest Batch      11 Loss : 0.617099488323385\nTest Batch      16 Loss : 0.6204107943922281\nTest Batch      21 Loss : 0.665680162963413\nTest Batch      26 Loss : 0.6436803054351073\nFor Epoch       6 Train Loss 0.639674 Train AUC 0.6863326741375522 Val Loss 0.701413145661354 Val AUC 0.6396370023419203 \nFor Epoch       6 Time Taken 35.31639680862427\nRunning Epoch 7...................\nTrain Batch       1 Loss : 0.5521290898323059 Time Taken : 0.15652767419815064 \nTrain Batch       6 Loss : 0.6686776876449585 Time Taken : 0.22020682096481323 \nTrain Batch      11 Loss : 0.6247071677988226 Time Taken : 0.216778834660848 \nTrain Batch      16 Loss : 0.6522470451891422 Time Taken : 0.21492758591969807 \nTrain Batch      21 Loss : 0.6274798994972592 Time Taken : 0.2145905574162801 \nTrain Batch      26 Loss : 0.6228552449208039 Time Taken : 0.21895511547724406 \nTrain Batch      31 Loss : 0.6169653977117231 Time Taken : 0.21653006871541342 \nTrain Batch      36 Loss : 0.6085396243466271 Time Taken : 0.21622902949651082 \nTrain Batch      41 Loss : 0.6107083224668736 Time Taken : 0.2205408811569214 \nTrain Batch      46 Loss : 0.6126624591972517 Time Taken : 0.21817773580551147 \nTrain Batch      51 Loss : 0.6131924425854403 Time Taken : 0.21598005294799805 \nTrain Batch      56 Loss : 0.6150151272969586 Time Taken : 0.21484267711639404 \nTrain Batch      61 Loss : 0.617929721953439 Time Taken : 0.2117767294247945 \nTrain Batch      66 Loss : 0.6172933050177314 Time Taken : 0.21819916168848674 \nTrain Batch      71 Loss : 0.6142318152206044 Time Taken : 0.2193296710650126 \nTrain Batch      76 Loss : 0.6124163995447912 Time Taken : 0.2161139408747355 \nTrain Batch      81 Loss : 0.6045784637515927 Time Taken : 0.21536871989568074 \nTrain Batch      86 Loss : 0.6057911192954972 Time Taken : 0.21622045040130616 \nTrain Batch      91 Loss : 0.6095850556106358 Time Taken : 0.2188927928606669 \nTrain Batch      96 Loss : 0.6123559552555283 Time Taken : 0.21644641558329264 \nTrain Batch     101 Loss : 0.6089889203557873 Time Taken : 0.2241721709569295 \nTrain Batch     106 Loss : 0.6191840185871664 Time Taken : 0.1810020407040914 \nTrain Batch     111 Loss : 0.6229622339880144 Time Taken : 0.18565109173456829 \nTrain Batch     116 Loss : 0.625183549420587 Time Taken : 0.18093525171279906 \nTest Batch       1 Loss : 0.485529363155365\nTest Batch       6 Loss : 0.6907590826352438\nTest Batch      11 Loss : 0.6309824016961184\nTest Batch      16 Loss : 0.6375543903559446\nTest Batch      21 Loss : 0.6966729689212072\nTest Batch      26 Loss : 0.6936092365246552\nFor Epoch       7 Train Loss 0.624757 Train AUC 0.7212334285505018 Val Loss 0.7205785681804021 Val AUC 0.5904566744730678 \nFor Epoch       7 Time Taken 41.03571041822433\nRunning Epoch 8...................\nTrain Batch       1 Loss : 0.5052904486656189 Time Taken : 0.16008315483729044 \nTrain Batch       6 Loss : 0.6597613493601481 Time Taken : 0.2144376794497172 \nTrain Batch      11 Loss : 0.6361786885695024 Time Taken : 0.22122631867726644 \nTrain Batch      16 Loss : 0.6049247551709414 Time Taken : 0.2174953540166219 \nTrain Batch      21 Loss : 0.6210943389506567 Time Taken : 0.2138554056485494 \nTrain Batch      26 Loss : 0.6283154636621475 Time Taken : 0.2203576644261678 \nTrain Batch      31 Loss : 0.6346360050862835 Time Taken : 0.21719382603963217 \nTrain Batch      36 Loss : 0.6295270828737153 Time Taken : 0.21315500736236573 \nTrain Batch      41 Loss : 0.6370635941261198 Time Taken : 0.22009638945261636 \nTrain Batch      46 Loss : 0.6395354782757552 Time Taken : 0.21249823967615764 \nTrain Batch      51 Loss : 0.6350782969418693 Time Taken : 0.21890790462493898 \nTrain Batch      56 Loss : 0.6336786081748349 Time Taken : 0.21594039996465048 \nTrain Batch      61 Loss : 0.630254899380637 Time Taken : 0.21600974798202516 \nTrain Batch      66 Loss : 0.629918069098935 Time Taken : 0.22545287211736043 \nTrain Batch      71 Loss : 0.6195384297572392 Time Taken : 0.2181127429008484 \nTrain Batch      76 Loss : 0.6220662366402777 Time Taken : 0.22211124897003173 \nTrain Batch      81 Loss : 0.6184794807139739 Time Taken : 0.21992431879043578 \nTrain Batch      86 Loss : 0.6184926992932032 Time Taken : 0.23041886885960897 \nTrain Batch      91 Loss : 0.6143803842120118 Time Taken : 0.21278042793273927 \nTrain Batch      96 Loss : 0.605751051257054 Time Taken : 0.2195480982462565 \nTrain Batch     101 Loss : 0.6074098832536452 Time Taken : 0.22926778793334962 \nTrain Batch     106 Loss : 0.6127806901931763 Time Taken : 0.18327734470367432 \nTrain Batch     111 Loss : 0.6093350154859526 Time Taken : 0.18287370602289835 \nTrain Batch     116 Loss : 0.6054290561326618 Time Taken : 0.1872479200363159 \nTest Batch       1 Loss : 0.5626505017280579\nTest Batch       6 Loss : 0.6626962423324585\nTest Batch      11 Loss : 0.655726736242121\nTest Batch      16 Loss : 0.6568942330777645\nTest Batch      21 Loss : 0.6927768361000788\nTest Batch      26 Loss : 0.6723555807883923\nFor Epoch       8 Train Loss 0.606772 Train AUC 0.7434995971581337 Val Loss 0.6928547362486521 Val AUC 0.5790398126463701 \nFor Epoch       8 Time Taken 46.79934637943904\nRunning Epoch 9...................\nTrain Batch       1 Loss : 0.4122743606567383 Time Taken : 0.14657992919286092 \nTrain Batch       6 Loss : 0.43862123787403107 Time Taken : 0.21461479663848876 \nTrain Batch      11 Loss : 0.5496530018069528 Time Taken : 0.2244676470756531 \nTrain Batch      16 Loss : 0.5609661992639303 Time Taken : 0.2199584682782491 \nTrain Batch      21 Loss : 0.6133779415062496 Time Taken : 0.21895625988642375 \nTrain Batch      26 Loss : 0.5877077831671789 Time Taken : 0.2219343066215515 \nTrain Batch      31 Loss : 0.5731005784003965 Time Taken : 0.21526867945988973 \nTrain Batch      36 Loss : 0.5572968266076512 Time Taken : 0.21461484829584757 \nTrain Batch      41 Loss : 0.5675688233317399 Time Taken : 0.220106037457784 \nTrain Batch      46 Loss : 0.565376184232857 Time Taken : 0.2202000101407369 \nTrain Batch      51 Loss : 0.5643206682859683 Time Taken : 0.2136823058128357 \nTrain Batch      56 Loss : 0.5687084118170398 Time Taken : 0.21170265674591066 \nTrain Batch      61 Loss : 0.5634194055541617 Time Taken : 0.21736318270365398 \nTrain Batch      66 Loss : 0.560978635242491 Time Taken : 0.21845717430114747 \nTrain Batch      71 Loss : 0.5683412614842536 Time Taken : 0.22053319613138836 \nTrain Batch      76 Loss : 0.5688164932163138 Time Taken : 0.21305044889450073 \nTrain Batch      81 Loss : 0.5673618165799129 Time Taken : 0.2171635389328003 \nTrain Batch      86 Loss : 0.5652032286621803 Time Taken : 0.222682253519694 \nTrain Batch      91 Loss : 0.5716243798916156 Time Taken : 0.22244869073232015 \nTrain Batch      96 Loss : 0.5721884689604243 Time Taken : 0.2227417786916097 \nTrain Batch     101 Loss : 0.5740692916482982 Time Taken : 0.21825668414433796 \nTrain Batch     106 Loss : 0.5735782637911023 Time Taken : 0.18768565654754638 \nTrain Batch     111 Loss : 0.5790003248163171 Time Taken : 0.18483461538950602 \nTrain Batch     116 Loss : 0.5808799701004193 Time Taken : 0.18302356799443562 \nTest Batch       1 Loss : 0.5772430896759033\nTest Batch       6 Loss : 0.6841736237208048\nTest Batch      11 Loss : 0.6549731303345073\nTest Batch      16 Loss : 0.6423591542989016\nTest Batch      21 Loss : 0.6828375870273227\nTest Batch      26 Loss : 0.6775545672728465\nFor Epoch       9 Train Loss 0.582214 Train AUC 0.7677799750970482 Val Loss 0.717134361465772 Val AUC 0.5764051522248242 \nFor Epoch       9 Time Taken 52.54174851973851\n\nValid auc didn't improve last 5 epochs.\n['T2w-e4-loss0.644-auc0.696.pth']\n","output_type":"stream"}]},{"cell_type":"code","source":"modelfiles = None\n\nif not modelfiles:\n    modelfiles = [train_mri_type(df_train, df_valid, m) for m in ['FLAIR']]\n    print(modelfiles)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T19:28:53.723574Z","iopub.execute_input":"2021-09-07T19:28:53.723985Z","iopub.status.idle":"2021-09-07T19:57:51.190944Z","shell.execute_reply.started":"2021-09-07T19:28:53.723953Z","shell.execute_reply":"2021-09-07T19:57:51.189893Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"(468, 3) (117, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     BraTS21ID  MGMT_value MRI_Type\n102        154           0    FLAIR\n161        240           1    FLAIR\n508        740           1    FLAIR\n495        725           1    FLAIR\n298        432           0    FLAIR","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BraTS21ID</th>\n      <th>MGMT_value</th>\n      <th>MRI_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>102</th>\n      <td>154</td>\n      <td>0</td>\n      <td>FLAIR</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>240</td>\n      <td>1</td>\n      <td>FLAIR</td>\n    </tr>\n    <tr>\n      <th>508</th>\n      <td>740</td>\n      <td>1</td>\n      <td>FLAIR</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>725</td>\n      <td>1</td>\n      <td>FLAIR</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>432</td>\n      <td>0</td>\n      <td>FLAIR</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Running Epoch 1...................\nTrain Batch       1 Loss : 0.71163409948349 Time Taken : 0.1697358767191569 \nTrain Batch       6 Loss : 1.0110363513231277 Time Taken : 0.23709946473439533 \nTrain Batch      11 Loss : 0.9293274473060261 Time Taken : 0.22152148087819418 \nTrain Batch      16 Loss : 0.8434413988143206 Time Taken : 0.22093075911204021 \nTrain Batch      21 Loss : 0.8398713795911699 Time Taken : 0.2208802580833435 \nTrain Batch      26 Loss : 0.8245215289867841 Time Taken : 0.22490954001744587 \nTrain Batch      31 Loss : 0.8067611926986326 Time Taken : 0.21855865319569906 \nTrain Batch      36 Loss : 0.7998089186019368 Time Taken : 0.2229761282602946 \nTrain Batch      41 Loss : 0.7967976629734039 Time Taken : 0.21913760503133137 \nTrain Batch      46 Loss : 0.7871002483627071 Time Taken : 0.22480460007985434 \nTrain Batch      51 Loss : 0.7774776962457919 Time Taken : 0.2247634808222453 \nTrain Batch      56 Loss : 0.7787573193865163 Time Taken : 0.21523717641830445 \nTrain Batch      61 Loss : 0.7756944469741134 Time Taken : 0.2153825879096985 \nTrain Batch      66 Loss : 0.7706243002956564 Time Taken : 0.21303761800130208 \nTrain Batch      71 Loss : 0.7683790573771571 Time Taken : 0.22373454173405966 \nTrain Batch      76 Loss : 0.7729652998478789 Time Taken : 0.21801663637161256 \nTrain Batch      81 Loss : 0.7666988229310071 Time Taken : 0.2160731514294942 \nTrain Batch      86 Loss : 0.7640881534925726 Time Taken : 0.2199657638867696 \nTrain Batch      91 Loss : 0.7593375735885495 Time Taken : 0.21000622510910033 \nTrain Batch      96 Loss : 0.7576697831973433 Time Taken : 0.2221690575281779 \nTrain Batch     101 Loss : 0.7540541730894901 Time Taken : 0.21501611471176146 \nTrain Batch     106 Loss : 0.7562577097483401 Time Taken : 0.18565951983133952 \nTrain Batch     111 Loss : 0.7522692564908449 Time Taken : 0.1838550090789795 \nTrain Batch     116 Loss : 0.7510192324889118 Time Taken : 0.18294617335001628 \nTest Batch       1 Loss : 0.5204704403877258\nTest Batch       6 Loss : 0.615710179011027\nTest Batch      11 Loss : 0.6640813838351857\nTest Batch      16 Loss : 0.6593374088406563\nTest Batch      21 Loss : 0.6636223395665487\nTest Batch      26 Loss : 0.66140766785695\nFor Epoch       1 Train Loss 0.751298 Train AUC 0.48295246465978175 Val Loss 0.661376777291298 Val AUC 0.6472482435597189 \nFor Epoch       1 Time Taken 5.850614770253499\nAUC Improved from  inf to 0.661376777291298. Saved model to FLAIR-e1-loss0.661-auc0.647.pth\nRunning Epoch 2...................\nTrain Batch       1 Loss : 0.7536190748214722 Time Taken : 0.17133630514144899 \nTrain Batch       6 Loss : 0.805911789337794 Time Taken : 0.21580482721328736 \nTrain Batch      11 Loss : 0.760833350094882 Time Taken : 0.21508601506551106 \nTrain Batch      16 Loss : 0.7418518364429474 Time Taken : 0.21937025388081868 \nTrain Batch      21 Loss : 0.7301980455716451 Time Taken : 0.21456553936004638 \nTrain Batch      26 Loss : 0.7187104729505686 Time Taken : 0.22336460749308268 \nTrain Batch      31 Loss : 0.7170563955460826 Time Taken : 0.22372990051905314 \nTrain Batch      36 Loss : 0.7260451018810272 Time Taken : 0.22486180861790975 \nTrain Batch      41 Loss : 0.7221689442308937 Time Taken : 0.216890545686086 \nTrain Batch      46 Loss : 0.7231224295885667 Time Taken : 0.21548279921213787 \nTrain Batch      51 Loss : 0.7213068639530855 Time Taken : 0.22240055799484254 \nTrain Batch      56 Loss : 0.7221227735280991 Time Taken : 0.2153117338816325 \nTrain Batch      61 Loss : 0.7259262116228948 Time Taken : 0.2191510836283366 \nTrain Batch      66 Loss : 0.7267661934549158 Time Taken : 0.21697014967600506 \nTrain Batch      71 Loss : 0.7244968305171375 Time Taken : 0.215341587861379 \nTrain Batch      76 Loss : 0.720952561811397 Time Taken : 0.2233214815457662 \nTrain Batch      81 Loss : 0.7230289306169675 Time Taken : 0.2214211384455363 \nTrain Batch      86 Loss : 0.7236389510853346 Time Taken : 0.2152783433596293 \nTrain Batch      91 Loss : 0.7178118025863564 Time Taken : 0.21263159513473512 \nTrain Batch      96 Loss : 0.7163096821556488 Time Taken : 0.22328831354777018 \nTrain Batch     101 Loss : 0.7182764322450845 Time Taken : 0.21941414674123128 \nTrain Batch     106 Loss : 0.7195101132932699 Time Taken : 0.1814355731010437 \nTrain Batch     111 Loss : 0.7168646084295737 Time Taken : 0.18129407962163288 \nTrain Batch     116 Loss : 0.715586589328174 Time Taken : 0.1874998132387797 \nTest Batch       1 Loss : 0.5600464344024658\nTest Batch       6 Loss : 0.6867401301860809\nTest Batch      11 Loss : 0.706320507959886\nTest Batch      16 Loss : 0.6924626342952251\nTest Batch      21 Loss : 0.697185666788192\nTest Batch      26 Loss : 0.6791766927792475\nFor Epoch       2 Train Loss 0.714933 Train AUC 0.5044495715227423 Val Loss 0.6965611775716146 Val AUC 0.5963114754098361 \nFor Epoch       2 Time Taken 11.62282734711965\nRunning Epoch 3...................\nTrain Batch       1 Loss : 0.6561771631240845 Time Taken : 0.1654070774714152 \nTrain Batch       6 Loss : 0.7256763378779093 Time Taken : 0.21984803279240925 \nTrain Batch      11 Loss : 0.7099570469422773 Time Taken : 0.2130300203959147 \nTrain Batch      16 Loss : 0.7027093358337879 Time Taken : 0.22158889770507811 \nTrain Batch      21 Loss : 0.6841096792902265 Time Taken : 0.21440006097157796 \nTrain Batch      26 Loss : 0.6837221086025238 Time Taken : 0.21393291552861532 \nTrain Batch      31 Loss : 0.6868252754211426 Time Taken : 0.21370229721069336 \nTrain Batch      36 Loss : 0.6898188574446572 Time Taken : 0.21960175434748333 \nTrain Batch      41 Loss : 0.6889531234415566 Time Taken : 0.2240133007367452 \nTrain Batch      46 Loss : 0.6990085296008898 Time Taken : 0.21605296134948732 \nTrain Batch      51 Loss : 0.6976932778077967 Time Taken : 0.22166412671407063 \nTrain Batch      56 Loss : 0.6990686591182437 Time Taken : 0.22525946696599325 \nTrain Batch      61 Loss : 0.6992575770518819 Time Taken : 0.2230020523071289 \nTrain Batch      66 Loss : 0.6954880934773069 Time Taken : 0.22087196906407675 \nTrain Batch      71 Loss : 0.6966378823132582 Time Taken : 0.21846941312154133 \nTrain Batch      76 Loss : 0.6960825127990622 Time Taken : 0.22251185178756713 \nTrain Batch      81 Loss : 0.6981217449094043 Time Taken : 0.2198629339536031 \nTrain Batch      86 Loss : 0.7011921419653782 Time Taken : 0.21735029617945353 \nTrain Batch      91 Loss : 0.7002969043595451 Time Taken : 0.22645236651102701 \nTrain Batch      96 Loss : 0.7018119065711895 Time Taken : 0.22206497589747112 \nTrain Batch     101 Loss : 0.6986673580537929 Time Taken : 0.22508718967437744 \nTrain Batch     106 Loss : 0.6988769921491731 Time Taken : 0.1912278970082601 \nTrain Batch     111 Loss : 0.6962753222869323 Time Taken : 0.18593793710072834 \nTrain Batch     116 Loss : 0.6964610434811691 Time Taken : 0.1890827258427938 \nTest Batch       1 Loss : 0.7174221277236938\nTest Batch       6 Loss : 0.7637163201967875\nTest Batch      11 Loss : 0.7313031283291903\nTest Batch      16 Loss : 0.7267948389053345\nTest Batch      21 Loss : 0.7215279369127183\nTest Batch      26 Loss : 0.7129854193100562\nFor Epoch       3 Train Loss 0.696588 Train AUC 0.5421335970116458 Val Loss 0.726408596833547 Val AUC 0.3966627634660421 \nFor Epoch       3 Time Taken 17.423528587818147\nRunning Epoch 4...................\nTrain Batch       1 Loss : 0.7308277487754822 Time Taken : 0.14959363937377929 \nTrain Batch       6 Loss : 0.6915749311447144 Time Taken : 0.2135396162668864 \nTrain Batch      11 Loss : 0.6618818640708923 Time Taken : 0.2193284551302592 \nTrain Batch      16 Loss : 0.6675694510340691 Time Taken : 0.21408507823944092 \nTrain Batch      21 Loss : 0.671908125990913 Time Taken : 0.22524398167928059 \nTrain Batch      26 Loss : 0.6745262421094455 Time Taken : 0.22125566403071087 \nTrain Batch      31 Loss : 0.6784342488934917 Time Taken : 0.21845592657725016 \nTrain Batch      36 Loss : 0.691871734129058 Time Taken : 0.2154767115910848 \nTrain Batch      41 Loss : 0.6937789509936076 Time Taken : 0.2180922508239746 \nTrain Batch      46 Loss : 0.6982670270878336 Time Taken : 0.21827332178751627 \nTrain Batch      51 Loss : 0.6984778327100417 Time Taken : 0.21656417449315388 \nTrain Batch      56 Loss : 0.6961341819592884 Time Taken : 0.22320371468861896 \nTrain Batch      61 Loss : 0.6952531699274407 Time Taken : 0.2193721850713094 \nTrain Batch      66 Loss : 0.6912762382716844 Time Taken : 0.21935863892237345 \nTrain Batch      71 Loss : 0.6956328485213535 Time Taken : 0.2209249496459961 \nTrain Batch      76 Loss : 0.7002499342748993 Time Taken : 0.21512250900268554 \nTrain Batch      81 Loss : 0.699690321713318 Time Taken : 0.22322899500528973 \nTrain Batch      86 Loss : 0.6986808253582134 Time Taken : 0.21712544361750286 \nTrain Batch      91 Loss : 0.6997389174424685 Time Taken : 0.22116872866948445 \nTrain Batch      96 Loss : 0.7005020631477237 Time Taken : 0.2242705225944519 \nTrain Batch     101 Loss : 0.7028749428763248 Time Taken : 0.22090349197387696 \nTrain Batch     106 Loss : 0.7022487405898437 Time Taken : 0.19568724234898885 \nTrain Batch     111 Loss : 0.6966218644971246 Time Taken : 0.1837297797203064 \nTrain Batch     116 Loss : 0.6976693614289678 Time Taken : 0.18313211997350057 \nTest Batch       1 Loss : 0.6640814542770386\nTest Batch       6 Loss : 0.6869308551152548\nTest Batch      11 Loss : 0.6719779480587352\nTest Batch      16 Loss : 0.6708124056458473\nTest Batch      21 Loss : 0.6792199015617371\nTest Batch      26 Loss : 0.680114399928313\nFor Epoch       4 Train Loss 0.697722 Train AUC 0.5263128982641179 Val Loss 0.6758324384689331 Val AUC 0.6238290398126464 \nFor Epoch       4 Time Taken 23.182850003242493\nRunning Epoch 5...................\nTrain Batch       1 Loss : 0.7861513495445251 Time Taken : 0.16015023390452068 \nTrain Batch       6 Loss : 0.7319802641868591 Time Taken : 0.2144718090693156 \nTrain Batch      11 Loss : 0.6957578686150637 Time Taken : 0.22082534631093342 \nTrain Batch      16 Loss : 0.6919313054531813 Time Taken : 0.2164891799290975 \nTrain Batch      21 Loss : 0.6704595259257725 Time Taken : 0.22329514424006144 \nTrain Batch      26 Loss : 0.6714735489625198 Time Taken : 0.21882604360580443 \nTrain Batch      31 Loss : 0.6740802391882865 Time Taken : 0.22376810709635417 \nTrain Batch      36 Loss : 0.671752338608106 Time Taken : 0.21856480439503986 \nTrain Batch      41 Loss : 0.6741003335976019 Time Taken : 0.22168794473012288 \nTrain Batch      46 Loss : 0.6750437189703402 Time Taken : 0.22188907861709595 \nTrain Batch      51 Loss : 0.6821628621980256 Time Taken : 0.21738163232803345 \nTrain Batch      56 Loss : 0.685413274381842 Time Taken : 0.21762981017430624 \nTrain Batch      61 Loss : 0.6828714882741209 Time Taken : 0.21951516071955363 \nTrain Batch      66 Loss : 0.6826850410663721 Time Taken : 0.21782960096995035 \nTrain Batch      71 Loss : 0.6817998348827093 Time Taken : 0.22034745613733928 \nTrain Batch      76 Loss : 0.6802378720358798 Time Taken : 0.22007819016774496 \nTrain Batch      81 Loss : 0.6814407088138439 Time Taken : 0.2234214941660563 \nTrain Batch      86 Loss : 0.6797158669593723 Time Taken : 0.2215950568517049 \nTrain Batch      91 Loss : 0.6796052678600772 Time Taken : 0.22580593824386597 \nTrain Batch      96 Loss : 0.6780784465372562 Time Taken : 0.22294883728027343 \nTrain Batch     101 Loss : 0.6740021859065141 Time Taken : 0.21829025745391845 \nTrain Batch     106 Loss : 0.6715261837783849 Time Taken : 0.18408434788386027 \nTrain Batch     111 Loss : 0.6804258267621737 Time Taken : 0.18776029348373413 \nTrain Batch     116 Loss : 0.6793297732184673 Time Taken : 0.18334494431813558 \nTest Batch       1 Loss : 0.7025325298309326\nTest Batch       6 Loss : 0.9157641331354777\nTest Batch      11 Loss : 0.8119935935193842\nTest Batch      16 Loss : 0.8050610162317753\nTest Batch      21 Loss : 0.8056973360833668\nTest Batch      26 Loss : 0.7619516436870282\nFor Epoch       5 Train Loss 0.679520 Train AUC 0.6091152127737494 Val Loss 0.8076770702997843 Val AUC 0.5322014051522248 \nFor Epoch       5 Time Taken 28.95319510300954\n['FLAIR-e1-loss0.661-auc0.647.pth']\n","output_type":"stream"}]},{"cell_type":"code","source":"modelfiles = None\n\nif not modelfiles:\n    modelfiles = [train_mri_type(df_train, df_valid, m) for m in ['T1wCE']]\n    print(modelfiles)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T15:42:52.575754Z","iopub.execute_input":"2021-09-07T15:42:52.576111Z","iopub.status.idle":"2021-09-07T16:27:27.310179Z","shell.execute_reply.started":"2021-09-07T15:42:52.576080Z","shell.execute_reply":"2021-09-07T16:27:27.308918Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"(468, 3) (117, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     BraTS21ID  MGMT_value MRI_Type\n102        154           0    T1wCE\n161        240           1    T1wCE\n508        740           1    T1wCE\n495        725           1    T1wCE\n298        432           0    T1wCE","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BraTS21ID</th>\n      <th>MGMT_value</th>\n      <th>MRI_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>102</th>\n      <td>154</td>\n      <td>0</td>\n      <td>T1wCE</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>240</td>\n      <td>1</td>\n      <td>T1wCE</td>\n    </tr>\n    <tr>\n      <th>508</th>\n      <td>740</td>\n      <td>1</td>\n      <td>T1wCE</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>725</td>\n      <td>1</td>\n      <td>T1wCE</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>432</td>\n      <td>0</td>\n      <td>T1wCE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Running Epoch 1...................\nTrain Batch       1 Loss : 0.7916409373283386 Time Taken : 0.16586863199869792 \nTrain Batch       6 Loss : 1.0633291006088257 Time Taken : 0.21725447177886964 \nTrain Batch      11 Loss : 1.037536539814689 Time Taken : 0.20504048268000286 \nTrain Batch      16 Loss : 1.0796618275344372 Time Taken : 0.21703360478083292 \nTrain Batch      21 Loss : 1.0720351082938058 Time Taken : 0.20683375199635823 \nTrain Batch      26 Loss : 1.0356693749244397 Time Taken : 0.21170257329940795 \nTrain Batch      31 Loss : 1.0021603876544583 Time Taken : 0.21460738976796467 \nTrain Batch      36 Loss : 0.9467438343498442 Time Taken : 0.2124841809272766 \nTrain Batch      41 Loss : 0.9174312425822746 Time Taken : 0.2143505573272705 \nTrain Batch      46 Loss : 0.8986812156179677 Time Taken : 0.21102457046508788 \nTrain Batch      51 Loss : 0.8799707644125995 Time Taken : 0.2090568423271179 \nTrain Batch      56 Loss : 0.8640023767948151 Time Taken : 0.21327878634134928 \nTrain Batch      61 Loss : 0.8532277160003537 Time Taken : 0.21358645757039388 \nTrain Batch      66 Loss : 0.8395777850440054 Time Taken : 0.21275397141774496 \nTrain Batch      71 Loss : 0.8255485900690858 Time Taken : 0.2132387638092041 \nTrain Batch      76 Loss : 0.8243748224095294 Time Taken : 0.2160712440808614 \nTrain Batch      81 Loss : 0.8138663761409712 Time Taken : 0.2139840284983317 \nTrain Batch      86 Loss : 0.804553666087084 Time Taken : 0.2157005270322164 \nTrain Batch      91 Loss : 0.7976533796761062 Time Taken : 0.21662540038426717 \nTrain Batch      96 Loss : 0.7937071745594343 Time Taken : 0.21519578695297242 \nTrain Batch     101 Loss : 0.7861655165653417 Time Taken : 0.2202059308687846 \nTrain Batch     106 Loss : 0.7840571009887839 Time Taken : 0.18057472705841066 \nTrain Batch     111 Loss : 0.7764531433045327 Time Taken : 0.18340744574864706 \nTrain Batch     116 Loss : 0.7770664882043312 Time Taken : 0.17899961074193318 \nTest Batch       1 Loss : 0.8557905554771423\nTest Batch       6 Loss : 0.7381393611431122\nTest Batch      11 Loss : 0.7202764695340936\nTest Batch      16 Loss : 0.7317151799798012\nTest Batch      21 Loss : 0.7371131039801098\nTest Batch      26 Loss : 0.7444109824987558\nFor Epoch       1 Train Loss 0.776883 Train AUC 0.5208928440635758 Val Loss 0.7508071879545848 Val AUC 0.47892271662763464 \nFor Epoch       1 Time Taken 5.654733272393544\nAUC Improved from  inf to 0.7508071879545848. Saved model to T1wCE-e1-loss0.751-auc0.479.pth\nRunning Epoch 2...................\nTrain Batch       1 Loss : 0.5828357934951782 Time Taken : 0.14670018355051676 \nTrain Batch       6 Loss : 0.6788261830806732 Time Taken : 0.21065726280212402 \nTrain Batch      11 Loss : 0.6626662774519487 Time Taken : 0.20788489977518718 \nTrain Batch      16 Loss : 0.654709704220295 Time Taken : 0.20559714635213217 \nTrain Batch      21 Loss : 0.6617122860181899 Time Taken : 0.21490070422490437 \nTrain Batch      26 Loss : 0.6560774674782386 Time Taken : 0.2032405098279317 \nTrain Batch      31 Loss : 0.6746066020381066 Time Taken : 0.20920070012410483 \nTrain Batch      36 Loss : 0.6723750581343969 Time Taken : 0.2132684111595154 \nTrain Batch      41 Loss : 0.6742805344302479 Time Taken : 0.20631921291351318 \nTrain Batch      46 Loss : 0.6710034867991572 Time Taken : 0.20554832617441812 \nTrain Batch      51 Loss : 0.6790564890001335 Time Taken : 0.20443170070648192 \nTrain Batch      56 Loss : 0.6801307632454804 Time Taken : 0.20724942684173583 \nTrain Batch      61 Loss : 0.688018981061998 Time Taken : 0.21387550433476765 \nTrain Batch      66 Loss : 0.6871698494210388 Time Taken : 0.21613821188608806 \nTrain Batch      71 Loss : 0.6871818484554828 Time Taken : 0.20717012882232666 \nTrain Batch      76 Loss : 0.6887229324171418 Time Taken : 0.20496936241785685 \nTrain Batch      81 Loss : 0.6925049795780653 Time Taken : 0.21434643268585205 \nTrain Batch      86 Loss : 0.6921538617028746 Time Taken : 0.20689537127812704 \nTrain Batch      91 Loss : 0.6892628574764336 Time Taken : 0.20859994490941366 \nTrain Batch      96 Loss : 0.6874118518705169 Time Taken : 0.20937938690185548 \nTrain Batch     101 Loss : 0.6864593940796239 Time Taken : 0.2057061831156413 \nTrain Batch     106 Loss : 0.6881954734617809 Time Taken : 0.1773702104886373 \nTrain Batch     111 Loss : 0.6917672519748276 Time Taken : 0.18165703614552817 \nTrain Batch     116 Loss : 0.6915109031673136 Time Taken : 0.17756895224253336 \nTest Batch       1 Loss : 0.8124858140945435\nTest Batch       6 Loss : 0.7972919444243113\nTest Batch      11 Loss : 0.7367514886639335\nTest Batch      16 Loss : 0.7388740796595812\nTest Batch      21 Loss : 0.747080724863779\nTest Batch      26 Loss : 0.7381962916025748\nFor Epoch       2 Train Loss 0.690952 Train AUC 0.5645279425767231 Val Loss 0.7449999680121739 Val AUC 0.39022248243559715 \nFor Epoch       2 Time Taken 11.16136117776235\nAUC Improved from 0.750807 to 0.7449999680121739. Saved model to T1wCE-e2-loss0.745-auc0.390.pth\nRunning Epoch 3...................\nTrain Batch       1 Loss : 0.7757835388183594 Time Taken : 0.14737266302108765 \nTrain Batch       6 Loss : 0.6823001950979233 Time Taken : 0.21708039045333863 \nTrain Batch      11 Loss : 0.7189019024372101 Time Taken : 0.20458614826202393 \nTrain Batch      16 Loss : 0.7137095462530851 Time Taken : 0.2078695257504781 \nTrain Batch      21 Loss : 0.6970388420990535 Time Taken : 0.213262943426768 \nTrain Batch      26 Loss : 0.7084901756965197 Time Taken : 0.20330838362375894 \nTrain Batch      31 Loss : 0.7114782612169942 Time Taken : 0.20384272336959838 \nTrain Batch      36 Loss : 0.7102588340640068 Time Taken : 0.2117123047510783 \nTrain Batch      41 Loss : 0.7094177291160677 Time Taken : 0.2155028740564982 \nTrain Batch      46 Loss : 0.7070965021848679 Time Taken : 0.20422683159510294 \nTrain Batch      51 Loss : 0.7045849298729616 Time Taken : 0.21535725593566896 \nTrain Batch      56 Loss : 0.6995230535311359 Time Taken : 0.21146320899327595 \nTrain Batch      61 Loss : 0.6959834054845279 Time Taken : 0.212688676516215 \nTrain Batch      66 Loss : 0.6975629415475961 Time Taken : 0.2114609162012736 \nTrain Batch      71 Loss : 0.6943851008381642 Time Taken : 0.20920917987823487 \nTrain Batch      76 Loss : 0.6900498463135016 Time Taken : 0.21141050259272257 \nTrain Batch      81 Loss : 0.6896909412778454 Time Taken : 0.20840805371602375 \nTrain Batch      86 Loss : 0.6904348491929299 Time Taken : 0.20571475028991698 \nTrain Batch      91 Loss : 0.6897842455041278 Time Taken : 0.21894699732462566 \nTrain Batch      96 Loss : 0.6879005509739121 Time Taken : 0.20291103521982828 \nTrain Batch     101 Loss : 0.687399562337611 Time Taken : 0.2130177855491638 \nTrain Batch     106 Loss : 0.6815600226510246 Time Taken : 0.1768735408782959 \nTrain Batch     111 Loss : 0.6787406552482296 Time Taken : 0.17605536778767902 \nTrain Batch     116 Loss : 0.6767737246279059 Time Taken : 0.17988969882329306 \nTest Batch       1 Loss : 0.5544672608375549\nTest Batch       6 Loss : 0.6830774744351705\nTest Batch      11 Loss : 0.7083824927156622\nTest Batch      16 Loss : 0.7206229493021965\nTest Batch      21 Loss : 0.7084241282372248\nTest Batch      26 Loss : 0.7018099335523752\nFor Epoch       3 Train Loss 0.676677 Train AUC 0.5957664982055226 Val Loss 0.6988790974020958 Val AUC 0.5052693208430914 \nFor Epoch       3 Time Taken 16.690782245000204\nAUC Improved from 0.745000 to 0.6988790974020958. Saved model to T1wCE-e3-loss0.699-auc0.505.pth\nRunning Epoch 4...................\nTrain Batch       1 Loss : 0.5850347280502319 Time Taken : 0.14383524258931477 \nTrain Batch       6 Loss : 0.6410434941450754 Time Taken : 0.2073192874590556 \nTrain Batch      11 Loss : 0.7622099356217817 Time Taken : 0.2164847175280253 \nTrain Batch      16 Loss : 0.7322691157460213 Time Taken : 0.20508950153986613 \nTrain Batch      21 Loss : 0.708821972211202 Time Taken : 0.21154272158940632 \nTrain Batch      26 Loss : 0.6951107634947851 Time Taken : 0.2059189001719157 \nTrain Batch      31 Loss : 0.6892543800415531 Time Taken : 0.2100137988726298 \nTrain Batch      36 Loss : 0.6964991291364034 Time Taken : 0.21979679266611735 \nTrain Batch      41 Loss : 0.695879686169508 Time Taken : 0.221015195051829 \nTrain Batch      46 Loss : 0.6934739195782206 Time Taken : 0.21069422165552776 \nTrain Batch      51 Loss : 0.6938503454713261 Time Taken : 0.21552608410517374 \nTrain Batch      56 Loss : 0.6827408341424805 Time Taken : 0.20820975303649902 \nTrain Batch      61 Loss : 0.6771963214287993 Time Taken : 0.2171740452448527 \nTrain Batch      66 Loss : 0.6759727692062204 Time Taken : 0.21174810727437338 \nTrain Batch      71 Loss : 0.6768271138130779 Time Taken : 0.21117772658665976 \nTrain Batch      76 Loss : 0.6743603321282488 Time Taken : 0.2166074792544047 \nTrain Batch      81 Loss : 0.6785888911029438 Time Taken : 0.21016358931859333 \nTrain Batch      86 Loss : 0.6777156435473021 Time Taken : 0.20915061632792156 \nTrain Batch      91 Loss : 0.6767790026062137 Time Taken : 0.21405319372812906 \nTrain Batch      96 Loss : 0.6732388290887078 Time Taken : 0.20571043093999228 \nTrain Batch     101 Loss : 0.6774646436223889 Time Taken : 0.21709471940994263 \nTrain Batch     106 Loss : 0.674810455936306 Time Taken : 0.17886410156885782 \nTrain Batch     111 Loss : 0.676060205375826 Time Taken : 0.18417366743087768 \nTrain Batch     116 Loss : 0.6799022953571945 Time Taken : 0.17825954357783 \nTest Batch       1 Loss : 0.7220375537872314\nTest Batch       6 Loss : 0.7280505100886027\nTest Batch      11 Loss : 0.6856221556663513\nTest Batch      16 Loss : 0.6979544498026371\nTest Batch      21 Loss : 0.7081902396111261\nTest Batch      26 Loss : 0.6939798857157047\nFor Epoch       4 Train Loss 0.679404 Train AUC 0.6054713249835201 Val Loss 0.7156992584466935 Val AUC 0.5207845433255269 \nFor Epoch       4 Time Taken 22.269071273008983\nRunning Epoch 5...................\nTrain Batch       1 Loss : 0.6065117716789246 Time Taken : 0.1418984889984131 \nTrain Batch       6 Loss : 0.6593501269817352 Time Taken : 0.2157788077990214 \nTrain Batch      11 Loss : 0.6523366136984392 Time Taken : 0.20998908678690592 \nTrain Batch      16 Loss : 0.6490628458559513 Time Taken : 0.21070392926534018 \nTrain Batch      21 Loss : 0.6385770298185802 Time Taken : 0.20813880364100137 \nTrain Batch      26 Loss : 0.6205539531432666 Time Taken : 0.20682339668273925 \nTrain Batch      31 Loss : 0.6131065535929895 Time Taken : 0.21259044408798217 \nTrain Batch      36 Loss : 0.6099256566829152 Time Taken : 0.20990770657857258 \nTrain Batch      41 Loss : 0.60777054545356 Time Taken : 0.20856884717941285 \nTrain Batch      46 Loss : 0.6173481552497201 Time Taken : 0.20771636168162028 \nTrain Batch      51 Loss : 0.6141109092562806 Time Taken : 0.2159153381983439 \nTrain Batch      56 Loss : 0.6205280518957547 Time Taken : 0.20708982149759927 \nTrain Batch      61 Loss : 0.6252729531194343 Time Taken : 0.2107863148053487 \nTrain Batch      66 Loss : 0.6255929664228902 Time Taken : 0.20816526412963868 \nTrain Batch      71 Loss : 0.6227523478823649 Time Taken : 0.21589060624440512 \nTrain Batch      76 Loss : 0.6370098563401323 Time Taken : 0.21665836572647096 \nTrain Batch      81 Loss : 0.6410380097818963 Time Taken : 0.2162465810775757 \nTrain Batch      86 Loss : 0.6507852940365325 Time Taken : 0.21505289872487385 \nTrain Batch      91 Loss : 0.6518380081915593 Time Taken : 0.2162154714266459 \nTrain Batch      96 Loss : 0.6511948586752018 Time Taken : 0.21084786256154378 \nTrain Batch     101 Loss : 0.6539685383881673 Time Taken : 0.21969738403956096 \nTrain Batch     106 Loss : 0.6553735890478458 Time Taken : 0.17819388707478842 \nTrain Batch     111 Loss : 0.6539418842341449 Time Taken : 0.1835763692855835 \nTrain Batch     116 Loss : 0.6526783920567611 Time Taken : 0.1795189658800761 \nTest Batch       1 Loss : 0.6246083378791809\nTest Batch       6 Loss : 0.7344414045413336\nTest Batch      11 Loss : 0.6976711885495619\nTest Batch      16 Loss : 0.7100416533648968\nTest Batch      21 Loss : 0.7098590760003953\nTest Batch      26 Loss : 0.6982188648902453\nFor Epoch       5 Train Loss 0.653655 Train AUC 0.665989159891599 Val Loss 0.7080874691406885 Val AUC 0.5324941451990632 \nFor Epoch       5 Time Taken 27.84349967241287\nRunning Epoch 6...................\nTrain Batch       1 Loss : 0.6352190375328064 Time Taken : 0.1490003228187561 \nTrain Batch       6 Loss : 0.5864728937546412 Time Taken : 0.2124977986017863 \nTrain Batch      11 Loss : 0.5903499180620367 Time Taken : 0.20739826758702595 \nTrain Batch      16 Loss : 0.6236187256872654 Time Taken : 0.2089463949203491 \nTrain Batch      21 Loss : 0.6219654863788968 Time Taken : 0.2102848211924235 \nTrain Batch      26 Loss : 0.6325619759467932 Time Taken : 0.20917731920878094 \nTrain Batch      31 Loss : 0.6312893744437925 Time Taken : 0.2174477259318034 \nTrain Batch      36 Loss : 0.6261394752396477 Time Taken : 0.2180871884028117 \nTrain Batch      41 Loss : 0.6220065718743859 Time Taken : 0.2143730362256368 \nTrain Batch      46 Loss : 0.6223183973975803 Time Taken : 0.2196855107943217 \nTrain Batch      51 Loss : 0.6171411199896943 Time Taken : 0.20573285818099976 \nTrain Batch      56 Loss : 0.6201883011630603 Time Taken : 0.2090972383817037 \nTrain Batch      61 Loss : 0.6142836535563234 Time Taken : 0.20786970853805542 \nTrain Batch      66 Loss : 0.617204436750123 Time Taken : 0.21158421834309896 \nTrain Batch      71 Loss : 0.6189383327121466 Time Taken : 0.2115062157313029 \nTrain Batch      76 Loss : 0.6221315892119157 Time Taken : 0.21161112785339356 \nTrain Batch      81 Loss : 0.6178444844705088 Time Taken : 0.21592561403910318 \nTrain Batch      86 Loss : 0.6227334578369939 Time Taken : 0.20734969774881998 \nTrain Batch      91 Loss : 0.61892025745832 Time Taken : 0.20935558080673217 \nTrain Batch      96 Loss : 0.6247321081658205 Time Taken : 0.2181386113166809 \nTrain Batch     101 Loss : 0.6275535739294373 Time Taken : 0.2139783263206482 \nTrain Batch     106 Loss : 0.6295075067934001 Time Taken : 0.1849551836649577 \nTrain Batch     111 Loss : 0.6320673334705937 Time Taken : 0.17823402484258016 \nTrain Batch     116 Loss : 0.6328400620098772 Time Taken : 0.17766707738240559 \nTest Batch       1 Loss : 0.7522252798080444\nTest Batch       6 Loss : 0.6658867200215658\nTest Batch      11 Loss : 0.7729025428945367\nTest Batch      16 Loss : 0.7716555222868919\nTest Batch      21 Loss : 0.7674526884442284\nTest Batch      26 Loss : 0.8325995092208569\nFor Epoch       6 Train Loss 0.632479 Train AUC 0.6923020581557167 Val Loss 0.7991445004940033 Val AUC 0.5450819672131147 \nFor Epoch       6 Time Taken 33.413857420285545\nRunning Epoch 7...................\nTrain Batch       1 Loss : 0.36505675315856934 Time Taken : 0.14925989309946697 \nTrain Batch       6 Loss : 0.5506158173084259 Time Taken : 0.21480521361033122 \nTrain Batch      11 Loss : 0.5688068081032146 Time Taken : 0.2062015732129415 \nTrain Batch      16 Loss : 0.5782221294939518 Time Taken : 0.21008034547170004 \nTrain Batch      21 Loss : 0.5915879380135309 Time Taken : 0.21333482265472412 \nTrain Batch      26 Loss : 0.5645682880511651 Time Taken : 0.21860556602478026 \nTrain Batch      31 Loss : 0.5722053473995578 Time Taken : 0.2118152658144633 \nTrain Batch      36 Loss : 0.5793634305397669 Time Taken : 0.20420926411946613 \nTrain Batch      41 Loss : 0.5785177175591631 Time Taken : 0.22077502806981406 \nTrain Batch      46 Loss : 0.577627683463304 Time Taken : 0.21213289896647136 \nTrain Batch      51 Loss : 0.57908201509831 Time Taken : 0.21523688236872354 \nTrain Batch      56 Loss : 0.5858302930636066 Time Taken : 0.21179157495498657 \nTrain Batch      61 Loss : 0.5887189272974358 Time Taken : 0.21124867995580038 \nTrain Batch      66 Loss : 0.5885297141291879 Time Taken : 0.21338703632354736 \nTrain Batch      71 Loss : 0.5958748119817653 Time Taken : 0.21589057842890422 \nTrain Batch      76 Loss : 0.6003461613466865 Time Taken : 0.20768425464630128 \nTrain Batch      81 Loss : 0.597041419994684 Time Taken : 0.2140926718711853 \nTrain Batch      86 Loss : 0.5934817607319632 Time Taken : 0.21486353874206543 \nTrain Batch      91 Loss : 0.592227795949349 Time Taken : 0.21183483203252157 \nTrain Batch      96 Loss : 0.5897141139333447 Time Taken : 0.21222888231277465 \nTrain Batch     101 Loss : 0.5863017870648073 Time Taken : 0.21192700862884523 \nTrain Batch     106 Loss : 0.586811399965916 Time Taken : 0.18344234625498454 \nTrain Batch     111 Loss : 0.5890236751990275 Time Taken : 0.18010255893071492 \nTrain Batch     116 Loss : 0.5914039254702371 Time Taken : 0.17936511834462485 \nTest Batch       1 Loss : 0.6329657435417175\nTest Batch       6 Loss : 0.6489072938760122\nTest Batch      11 Loss : 0.6957446499304338\nTest Batch      16 Loss : 0.7276614531874657\nTest Batch      21 Loss : 0.7256783417293003\nTest Batch      26 Loss : 0.7468557059764862\nFor Epoch       7 Train Loss 0.590584 Train AUC 0.7679997070240971 Val Loss 0.7215996950864791 Val AUC 0.5362997658079626 \nFor Epoch       7 Time Taken 39.00469402472178\nRunning Epoch 8...................\nTrain Batch       1 Loss : 0.2130439728498459 Time Taken : 0.1604637583096822 \nTrain Batch       6 Loss : 0.4948211486140887 Time Taken : 0.20965358813603718 \nTrain Batch      11 Loss : 0.5525886375795711 Time Taken : 0.21188614765803018 \nTrain Batch      16 Loss : 0.5277840988710523 Time Taken : 0.21056249539057414 \nTrain Batch      21 Loss : 0.5471720617441904 Time Taken : 0.20758024454116822 \nTrain Batch      26 Loss : 0.5636734601396781 Time Taken : 0.2189403732617696 \nTrain Batch      31 Loss : 0.5828000369571871 Time Taken : 0.21106333335240682 \nTrain Batch      36 Loss : 0.5831854372388787 Time Taken : 0.20488811731338502 \nTrain Batch      41 Loss : 0.5727704578056568 Time Taken : 0.21637693246205647 \nTrain Batch      46 Loss : 0.5802823671180269 Time Taken : 0.21275881926218668 \nTrain Batch      51 Loss : 0.5630700833657208 Time Taken : 0.2170558015505473 \nTrain Batch      56 Loss : 0.5659471111638206 Time Taken : 0.21613489389419555 \nTrain Batch      61 Loss : 0.5642206766566292 Time Taken : 0.21021358569463094 \nTrain Batch      66 Loss : 0.568286090186148 Time Taken : 0.21440908908843995 \nTrain Batch      71 Loss : 0.5637316254662795 Time Taken : 0.21202935775121054 \nTrain Batch      76 Loss : 0.5580314927195248 Time Taken : 0.210768727461497 \nTrain Batch      81 Loss : 0.5624458885487215 Time Taken : 0.2098821759223938 \nTrain Batch      86 Loss : 0.5530543954566468 Time Taken : 0.20925795237223307 \nTrain Batch      91 Loss : 0.5616916159352103 Time Taken : 0.20758105119069417 \nTrain Batch      96 Loss : 0.558554298710078 Time Taken : 0.21022233565648396 \nTrain Batch     101 Loss : 0.5546025815281538 Time Taken : 0.2112444003423055 \nTrain Batch     106 Loss : 0.5621699222697402 Time Taken : 0.17929648160934447 \nTrain Batch     111 Loss : 0.5590236145365346 Time Taken : 0.18054654598236083 \nTrain Batch     116 Loss : 0.5573176072332365 Time Taken : 0.18197530508041382 \nTest Batch       1 Loss : 0.6597967147827148\nTest Batch       6 Loss : 0.6804209848244985\nTest Batch      11 Loss : 0.6686977798288519\nTest Batch      16 Loss : 0.6784777492284775\nTest Batch      21 Loss : 0.6981612074942816\nTest Batch      26 Loss : 0.7098442545303931\nFor Epoch       8 Train Loss 0.558293 Train AUC 0.7947337581483923 Val Loss 0.704117351770401 Val AUC 0.5623536299765808 \nFor Epoch       8 Time Taken 44.57355157534281\n\nValid auc didn't improve last 5 epochs.\n['T1wCE-e3-loss0.699-auc0.505.pth']\n","output_type":"stream"}]},{"cell_type":"code","source":"modelfiles = None\n\nif not modelfiles:\n    modelfiles = [train_mri_type(df_train, df_valid, m) for m in ['T1w']]\n    print(modelfiles)","metadata":{"execution":{"iopub.status.busy":"2021-09-07T16:42:22.288447Z","iopub.execute_input":"2021-09-07T16:42:22.288846Z","iopub.status.idle":"2021-09-07T17:38:23.610410Z","shell.execute_reply.started":"2021-09-07T16:42:22.288811Z","shell.execute_reply":"2021-09-07T17:38:23.609487Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"(468, 3) (117, 3)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     BraTS21ID  MGMT_value MRI_Type\n102        154           0      T1w\n161        240           1      T1w\n508        740           1      T1w\n495        725           1      T1w\n298        432           0      T1w","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>BraTS21ID</th>\n      <th>MGMT_value</th>\n      <th>MRI_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>102</th>\n      <td>154</td>\n      <td>0</td>\n      <td>T1w</td>\n    </tr>\n    <tr>\n      <th>161</th>\n      <td>240</td>\n      <td>1</td>\n      <td>T1w</td>\n    </tr>\n    <tr>\n      <th>508</th>\n      <td>740</td>\n      <td>1</td>\n      <td>T1w</td>\n    </tr>\n    <tr>\n      <th>495</th>\n      <td>725</td>\n      <td>1</td>\n      <td>T1w</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>432</td>\n      <td>0</td>\n      <td>T1w</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Running Epoch 1...................\nTrain Batch       1 Loss : 0.602473795413971 Time Taken : 0.14525330861409505 \nTrain Batch       6 Loss : 0.6718316972255707 Time Taken : 0.22649770180384318 \nTrain Batch      11 Loss : 0.8280773758888245 Time Taken : 0.2129115343093872 \nTrain Batch      16 Loss : 0.7965491265058517 Time Taken : 0.21849509477615356 \nTrain Batch      21 Loss : 0.7694494639124189 Time Taken : 0.22142302989959717 \nTrain Batch      26 Loss : 0.763761735879458 Time Taken : 0.2125857671101888 \nTrain Batch      31 Loss : 0.7563709597433766 Time Taken : 0.2164621313412984 \nTrain Batch      36 Loss : 0.7607561498880386 Time Taken : 0.2196936011314392 \nTrain Batch      41 Loss : 0.7585284768081293 Time Taken : 0.21788819630940756 \nTrain Batch      46 Loss : 0.7446494750354601 Time Taken : 0.21289621988932292 \nTrain Batch      51 Loss : 0.7366472354122237 Time Taken : 0.20841342210769653 \nTrain Batch      56 Loss : 0.7359764501452446 Time Taken : 0.2207354744275411 \nTrain Batch      61 Loss : 0.7387588151165696 Time Taken : 0.21480220556259155 \nTrain Batch      66 Loss : 0.7317839002970493 Time Taken : 0.21831717491149902 \nTrain Batch      71 Loss : 0.7386892992006221 Time Taken : 0.21387966076533 \nTrain Batch      76 Loss : 0.7364281204185987 Time Taken : 0.21481890281041463 \nTrain Batch      81 Loss : 0.7391550688096035 Time Taken : 0.22051885922749836 \nTrain Batch      86 Loss : 0.7387853257877882 Time Taken : 0.21803772449493408 \nTrain Batch      91 Loss : 0.7351830149744893 Time Taken : 0.2203460415204366 \nTrain Batch      96 Loss : 0.7348896730691195 Time Taken : 0.21373369693756103 \nTrain Batch     101 Loss : 0.7336553760094218 Time Taken : 0.21909089088439943 \nTrain Batch     106 Loss : 0.7343110397176923 Time Taken : 0.1933314283688863 \nTrain Batch     111 Loss : 0.7333254185882775 Time Taken : 0.18271698156992594 \nTrain Batch     116 Loss : 0.733587629322348 Time Taken : 0.18402640422185262 \nTest Batch       1 Loss : 0.735451340675354\nTest Batch       6 Loss : 0.7656420369942983\nTest Batch      11 Loss : 0.7607215697115118\nTest Batch      16 Loss : 0.797567717730999\nTest Batch      21 Loss : 0.7978618116605849\nTest Batch      26 Loss : 0.7807038724422455\nFor Epoch       1 Train Loss 0.733572 Train AUC 0.5399728997289973 Val Loss 0.8023290991783142 Val AUC 0.3744145199063232 \nFor Epoch       1 Time Taken 5.708166122436523\nAUC Improved from  inf to 0.8023290991783142. Saved model to T1w-e1-loss0.802-auc0.374.pth\nRunning Epoch 2...................\nTrain Batch       1 Loss : 0.5864074230194092 Time Taken : 0.14566479523976644 \nTrain Batch       6 Loss : 0.6745266119639078 Time Taken : 0.21343696117401123 \nTrain Batch      11 Loss : 0.6690989353440024 Time Taken : 0.2203815778096517 \nTrain Batch      16 Loss : 0.6777287498116493 Time Taken : 0.21026761531829835 \nTrain Batch      21 Loss : 0.6999960853939965 Time Taken : 0.21490085522333782 \nTrain Batch      26 Loss : 0.7179996715142176 Time Taken : 0.2180603822072347 \nTrain Batch      31 Loss : 0.7197519367741 Time Taken : 0.21915760040283203 \nTrain Batch      36 Loss : 0.7125257419215308 Time Taken : 0.22070869207382202 \nTrain Batch      41 Loss : 0.7038241915586518 Time Taken : 0.2159865935643514 \nTrain Batch      46 Loss : 0.7063638166241024 Time Taken : 0.22632557551066082 \nTrain Batch      51 Loss : 0.711437020816055 Time Taken : 0.2295024037361145 \nTrain Batch      56 Loss : 0.714602142572403 Time Taken : 0.22692944208780924 \nTrain Batch      61 Loss : 0.7143374515361474 Time Taken : 0.2149591326713562 \nTrain Batch      66 Loss : 0.717418706778324 Time Taken : 0.22558940251668294 \nTrain Batch      71 Loss : 0.7220964817933633 Time Taken : 0.22473392883936563 \nTrain Batch      76 Loss : 0.7296643539478904 Time Taken : 0.2146084229151408 \nTrain Batch      81 Loss : 0.728607996010486 Time Taken : 0.2255117376645406 \nTrain Batch      86 Loss : 0.7276209565096123 Time Taken : 0.22197821935017903 \nTrain Batch      91 Loss : 0.7264692613056728 Time Taken : 0.21760842005411785 \nTrain Batch      96 Loss : 0.7224265194187561 Time Taken : 0.2209524194399516 \nTrain Batch     101 Loss : 0.7206189266525873 Time Taken : 0.2175032655398051 \nTrain Batch     106 Loss : 0.7199425247480284 Time Taken : 0.18820989926656087 \nTrain Batch     111 Loss : 0.7172641324567365 Time Taken : 0.18830934365590413 \nTrain Batch     116 Loss : 0.7180068965615898 Time Taken : 0.18815722465515136 \nTest Batch       1 Loss : 0.80450439453125\nTest Batch       6 Loss : 0.7434091667334238\nTest Batch      11 Loss : 0.7521098472855308\nTest Batch      16 Loss : 0.7395449578762054\nTest Batch      21 Loss : 0.7510159384636652\nTest Batch      26 Loss : 0.7485206585664016\nFor Epoch       2 Train Loss 0.718146 Train AUC 0.49309675529187724 Val Loss 0.7397020171085994 Val AUC 0.40661592505854793 \nFor Epoch       2 Time Taken 11.472096308072407\nAUC Improved from 0.802329 to 0.7397020171085994. Saved model to T1w-e2-loss0.740-auc0.407.pth\nRunning Epoch 3...................\nTrain Batch       1 Loss : 0.7931405305862427 Time Taken : 0.13897340695063273 \nTrain Batch       6 Loss : 0.7336988349755605 Time Taken : 0.21889535586039224 \nTrain Batch      11 Loss : 0.7157441919500177 Time Taken : 0.21254615783691405 \nTrain Batch      16 Loss : 0.7141155377030373 Time Taken : 0.2071594794591268 \nTrain Batch      21 Loss : 0.7167157530784607 Time Taken : 0.21163934866587322 \nTrain Batch      26 Loss : 0.7192546610648816 Time Taken : 0.21397682030995688 \nTrain Batch      31 Loss : 0.7070204807866004 Time Taken : 0.21304808855056762 \nTrain Batch      36 Loss : 0.708098989393976 Time Taken : 0.21361101071039837 \nTrain Batch      41 Loss : 0.7019528833831229 Time Taken : 0.21042659680048625 \nTrain Batch      46 Loss : 0.6998050368350485 Time Taken : 0.2154048999150594 \nTrain Batch      51 Loss : 0.6980134611036263 Time Taken : 0.21484632889429728 \nTrain Batch      56 Loss : 0.6957065803664071 Time Taken : 0.21626366376876832 \nTrain Batch      61 Loss : 0.6989495900810742 Time Taken : 0.21157963673273722 \nTrain Batch      66 Loss : 0.7001824848579637 Time Taken : 0.2115055720011393 \nTrain Batch      71 Loss : 0.7030008065868432 Time Taken : 0.2234527269999186 \nTrain Batch      76 Loss : 0.6964946953873885 Time Taken : 0.21911821762720743 \nTrain Batch      81 Loss : 0.6963772435247162 Time Taken : 0.2123328725496928 \nTrain Batch      86 Loss : 0.6933471359485803 Time Taken : 0.21595099369684856 \nTrain Batch      91 Loss : 0.6983184414905507 Time Taken : 0.21236380338668823 \nTrain Batch      96 Loss : 0.6962845933934053 Time Taken : 0.2167814056078593 \nTrain Batch     101 Loss : 0.6954807285035011 Time Taken : 0.22628414630889893 \nTrain Batch     106 Loss : 0.6946532169602951 Time Taken : 0.2017814040184021 \nTrain Batch     111 Loss : 0.6953738750638189 Time Taken : 0.2004266341527303 \nTrain Batch     116 Loss : 0.6978926889855286 Time Taken : 0.19039786259333294 \nTest Batch       1 Loss : 0.8527841567993164\nTest Batch       6 Loss : 0.7541833221912384\nTest Batch      11 Loss : 0.7501587000760165\nTest Batch      16 Loss : 0.736736137419939\nTest Batch      21 Loss : 0.7493347156615484\nTest Batch      26 Loss : 0.7412136540963099\nFor Epoch       3 Train Loss 0.698941 Train AUC 0.5411264923460046 Val Loss 0.7541720330715179 Val AUC 0.4364754098360656 \nFor Epoch       3 Time Taken 17.163322687149048\nRunning Epoch 4...................\nTrain Batch       1 Loss : 0.8135831952095032 Time Taken : 0.13626780112584433 \nTrain Batch       6 Loss : 0.6943114797274271 Time Taken : 0.21328742504119874 \nTrain Batch      11 Loss : 0.6830528432672675 Time Taken : 0.2268586277961731 \nTrain Batch      16 Loss : 0.6883792467415333 Time Taken : 0.23232335249582928 \nTrain Batch      21 Loss : 0.7049026971771604 Time Taken : 0.21518578131993613 \nTrain Batch      26 Loss : 0.696952047256323 Time Taken : 0.2161315401395162 \nTrain Batch      31 Loss : 0.6871662236029102 Time Taken : 0.2194956660270691 \nTrain Batch      36 Loss : 0.682458746764395 Time Taken : 0.21854068438212076 \nTrain Batch      41 Loss : 0.6828644886249449 Time Taken : 0.2153512994448344 \nTrain Batch      46 Loss : 0.6844081930492235 Time Taken : 0.21154048442840576 \nTrain Batch      51 Loss : 0.6881734097705168 Time Taken : 0.211252494653066 \nTrain Batch      56 Loss : 0.683779528098447 Time Taken : 0.21111737887064616 \nTrain Batch      61 Loss : 0.6849032112809478 Time Taken : 0.2161371072133382 \nTrain Batch      66 Loss : 0.6850723497795336 Time Taken : 0.21292676528294882 \nTrain Batch      71 Loss : 0.6817869745509725 Time Taken : 0.21249138911565144 \nTrain Batch      76 Loss : 0.6802798493912345 Time Taken : 0.21400386889775594 \nTrain Batch      81 Loss : 0.6797861633477388 Time Taken : 0.21490348974863688 \nTrain Batch      86 Loss : 0.6769749813301619 Time Taken : 0.2115942398707072 \nTrain Batch      91 Loss : 0.6737356847459144 Time Taken : 0.21993282238642375 \nTrain Batch      96 Loss : 0.6799963464339575 Time Taken : 0.21190316279729207 \nTrain Batch     101 Loss : 0.6798717721854106 Time Taken : 0.21746861934661865 \nTrain Batch     106 Loss : 0.6809151459415004 Time Taken : 0.1867923895517985 \nTrain Batch     111 Loss : 0.6790547929368578 Time Taken : 0.18716387351353964 \nTrain Batch     116 Loss : 0.6799490205172835 Time Taken : 0.1891276756922404 \nTest Batch       1 Loss : 0.6092814207077026\nTest Batch       6 Loss : 0.6657575269540151\nTest Batch      11 Loss : 0.6806335395032709\nTest Batch      16 Loss : 0.6941997408866882\nTest Batch      21 Loss : 0.7036963275500706\nTest Batch      26 Loss : 0.6959339013466468\nFor Epoch       4 Train Loss 0.679957 Train AUC 0.5928367391782026 Val Loss 0.7063288668791453 Val AUC 0.5128805620608899 \nFor Epoch       4 Time Taken 22.814357185363768\nAUC Improved from 0.739702 to 0.7063288668791453. Saved model to T1w-e4-loss0.706-auc0.513.pth\nRunning Epoch 5...................\nTrain Batch       1 Loss : 0.874268651008606 Time Taken : 0.13227945168813068 \nTrain Batch       6 Loss : 0.7098771035671234 Time Taken : 0.20949376424153646 \nTrain Batch      11 Loss : 0.7065976370464672 Time Taken : 0.21015325387318928 \nTrain Batch      16 Loss : 0.7168548330664635 Time Taken : 0.211710520585378 \nTrain Batch      21 Loss : 0.7003321732793536 Time Taken : 0.21633622248967488 \nTrain Batch      26 Loss : 0.6995552411446204 Time Taken : 0.21745871702829997 \nTrain Batch      31 Loss : 0.7016208594845187 Time Taken : 0.20786399841308595 \nTrain Batch      36 Loss : 0.6926432963874605 Time Taken : 0.20429986715316772 \nTrain Batch      41 Loss : 0.7013951205625767 Time Taken : 0.21237653493881226 \nTrain Batch      46 Loss : 0.6976657017417576 Time Taken : 0.21133989890416463 \nTrain Batch      51 Loss : 0.7005590247172936 Time Taken : 0.21060598691304525 \nTrain Batch      56 Loss : 0.6934538036584854 Time Taken : 0.20758343935012818 \nTrain Batch      61 Loss : 0.6885589742269672 Time Taken : 0.21218996047973632 \nTrain Batch      66 Loss : 0.689702076442314 Time Taken : 0.2128744920094808 \nTrain Batch      71 Loss : 0.6989620856835809 Time Taken : 0.21058685382207235 \nTrain Batch      76 Loss : 0.6924132591015414 Time Taken : 0.21259560187657675 \nTrain Batch      81 Loss : 0.6877887112858855 Time Taken : 0.21257520119349163 \nTrain Batch      86 Loss : 0.6825542980155279 Time Taken : 0.21536523898442586 \nTrain Batch      91 Loss : 0.6815288620335715 Time Taken : 0.22021230459213256 \nTrain Batch      96 Loss : 0.6803709141289195 Time Taken : 0.21648334662119548 \nTrain Batch     101 Loss : 0.6817297944338014 Time Taken : 0.2085021456082662 \nTrain Batch     106 Loss : 0.685457688176407 Time Taken : 0.18332455158233643 \nTrain Batch     111 Loss : 0.6877146607583707 Time Taken : 0.1864877661069234 \nTrain Batch     116 Loss : 0.6882144220430275 Time Taken : 0.1818499445915222 \nTest Batch       1 Loss : 0.791178822517395\nTest Batch       6 Loss : 0.7233796219031016\nTest Batch      11 Loss : 0.7604644894599915\nTest Batch      16 Loss : 0.7547570541501045\nTest Batch      21 Loss : 0.7610109079451788\nTest Batch      26 Loss : 0.7640778353581061\nFor Epoch       5 Train Loss 0.689141 Train AUC 0.5779132791327913 Val Loss 0.7601024309794108 Val AUC 0.3990046838407494 \nFor Epoch       5 Time Taken 28.37308820883433\nRunning Epoch 6...................\nTrain Batch       1 Loss : 0.7022901177406311 Time Taken : 0.1309304396311442 \nTrain Batch       6 Loss : 0.7176021834214529 Time Taken : 0.21065590381622315 \nTrain Batch      11 Loss : 0.6929698532277887 Time Taken : 0.20668524503707886 \nTrain Batch      16 Loss : 0.6955269500613213 Time Taken : 0.213125479221344 \nTrain Batch      21 Loss : 0.6904956897099813 Time Taken : 0.21080238819122316 \nTrain Batch      26 Loss : 0.6916482104704931 Time Taken : 0.21846567789713542 \nTrain Batch      31 Loss : 0.6864211155522254 Time Taken : 0.20916387637456257 \nTrain Batch      36 Loss : 0.6975737694236968 Time Taken : 0.20870533386866252 \nTrain Batch      41 Loss : 0.6876496163810172 Time Taken : 0.20436012347539265 \nTrain Batch      46 Loss : 0.6799332888230033 Time Taken : 0.21023613214492798 \nTrain Batch      51 Loss : 0.6827829398360907 Time Taken : 0.20672450065612794 \nTrain Batch      56 Loss : 0.6845059543848038 Time Taken : 0.21446130673090616 \nTrain Batch      61 Loss : 0.6822735144466651 Time Taken : 0.20893208185831705 \nTrain Batch      66 Loss : 0.6857598148512117 Time Taken : 0.21297024885813395 \nTrain Batch      71 Loss : 0.694156828480707 Time Taken : 0.21761216322580973 \nTrain Batch      76 Loss : 0.6903364458366444 Time Taken : 0.21154170831044514 \nTrain Batch      81 Loss : 0.6868508207945176 Time Taken : 0.2169438640276591 \nTrain Batch      86 Loss : 0.6907658348249834 Time Taken : 0.2129731019337972 \nTrain Batch      91 Loss : 0.6939563849470117 Time Taken : 0.20795804659525555 \nTrain Batch      96 Loss : 0.6910853615651528 Time Taken : 0.21193291346232096 \nTrain Batch     101 Loss : 0.6890780424127484 Time Taken : 0.210947052637736 \nTrain Batch     106 Loss : 0.68895737461324 Time Taken : 0.1870203177134196 \nTrain Batch     111 Loss : 0.6843864149875469 Time Taken : 0.18012528419494628 \nTrain Batch     116 Loss : 0.6860313811178865 Time Taken : 0.18293701410293578 \nTest Batch       1 Loss : 0.5752942562103271\nTest Batch       6 Loss : 0.6370993455251058\nTest Batch      11 Loss : 0.659122412854975\nTest Batch      16 Loss : 0.6761571392416954\nTest Batch      21 Loss : 0.6878464236145928\nTest Batch      26 Loss : 0.679355514737276\nFor Epoch       6 Train Loss 0.684527 Train AUC 0.5778034131692669 Val Loss 0.6901449392239253 Val AUC 0.5723067915690867 \nFor Epoch       6 Time Taken 33.906665953000385\nAUC Improved from 0.706329 to 0.6901449392239253. Saved model to T1w-e6-loss0.690-auc0.572.pth\nRunning Epoch 7...................\nTrain Batch       1 Loss : 0.8259626030921936 Time Taken : 0.13837837378184 \nTrain Batch       6 Loss : 0.6904109021027883 Time Taken : 0.20937163829803468 \nTrain Batch      11 Loss : 0.6721239956942472 Time Taken : 0.20720027287801107 \nTrain Batch      16 Loss : 0.6654237471520901 Time Taken : 0.21608190536499022 \nTrain Batch      21 Loss : 0.6657905181248983 Time Taken : 0.21443871259689332 \nTrain Batch      26 Loss : 0.6594818234443665 Time Taken : 0.21525472402572632 \nTrain Batch      31 Loss : 0.6767415856161425 Time Taken : 0.2084035833676656 \nTrain Batch      36 Loss : 0.67784909490082 Time Taken : 0.20964598258336384 \nTrain Batch      41 Loss : 0.6704212928690562 Time Taken : 0.21459062894185385 \nTrain Batch      46 Loss : 0.6833879021198853 Time Taken : 0.21262038151423138 \nTrain Batch      51 Loss : 0.6778783511881735 Time Taken : 0.21127756436665854 \nTrain Batch      56 Loss : 0.68181408356343 Time Taken : 0.20937127272288006 \nTrain Batch      61 Loss : 0.6775407151120608 Time Taken : 0.20848946968714396 \nTrain Batch      66 Loss : 0.6805487261577086 Time Taken : 0.2201684832572937 \nTrain Batch      71 Loss : 0.6754554875299964 Time Taken : 0.2132597327232361 \nTrain Batch      76 Loss : 0.6706193842385945 Time Taken : 0.21381364663441976 \nTrain Batch      81 Loss : 0.6738164836977735 Time Taken : 0.20903160174687704 \nTrain Batch      86 Loss : 0.6683007894560348 Time Taken : 0.21013969977696736 \nTrain Batch      91 Loss : 0.6726963310451298 Time Taken : 0.21434350411097208 \nTrain Batch      96 Loss : 0.6703746371592084 Time Taken : 0.21290135383605957 \nTrain Batch     101 Loss : 0.6722198662781479 Time Taken : 0.20733966827392578 \nTrain Batch     106 Loss : 0.672740194876239 Time Taken : 0.1823251525561015 \nTrain Batch     111 Loss : 0.6728792241564742 Time Taken : 0.1831473429997762 \nTrain Batch     116 Loss : 0.6736228396666462 Time Taken : 0.18140087525049844 \nTest Batch       1 Loss : 0.673554003238678\nTest Batch       6 Loss : 0.666233703494072\nTest Batch      11 Loss : 0.7051318260756406\nTest Batch      16 Loss : 0.723987715318799\nTest Batch      21 Loss : 0.722641898053033\nTest Batch      26 Loss : 0.7246395636063355\nFor Epoch       7 Train Loss 0.675288 Train AUC 0.6126309236065334 Val Loss 0.7265664070844651 Val AUC 0.42418032786885246 \nFor Epoch       7 Time Taken 39.45630399386088\nRunning Epoch 8...................\nTrain Batch       1 Loss : 0.7732183933258057 Time Taken : 0.133014182249705 \nTrain Batch       6 Loss : 0.6979920168717703 Time Taken : 0.20831958452860513 \nTrain Batch      11 Loss : 0.6369442018595609 Time Taken : 0.21292006174723307 \nTrain Batch      16 Loss : 0.6667422987520695 Time Taken : 0.20785980224609374 \nTrain Batch      21 Loss : 0.6656599300248283 Time Taken : 0.20809452931086223 \nTrain Batch      26 Loss : 0.6631949429328625 Time Taken : 0.20538279215494792 \nTrain Batch      31 Loss : 0.659537193275267 Time Taken : 0.20460137923558552 \nTrain Batch      36 Loss : 0.6671870276331902 Time Taken : 0.21260991891225178 \nTrain Batch      41 Loss : 0.681485883346418 Time Taken : 0.20972073475519817 \nTrain Batch      46 Loss : 0.6757148070179898 Time Taken : 0.20579657951990762 \nTrain Batch      51 Loss : 0.6794583955231834 Time Taken : 0.21775360107421876 \nTrain Batch      56 Loss : 0.6769299586968762 Time Taken : 0.2063005844751994 \nTrain Batch      61 Loss : 0.6717383377864713 Time Taken : 0.21007106304168702 \nTrain Batch      66 Loss : 0.6729979636994275 Time Taken : 0.21272833744684855 \nTrain Batch      71 Loss : 0.6694370268935889 Time Taken : 0.21369719505310059 \nTrain Batch      76 Loss : 0.6709799182258154 Time Taken : 0.21494765679041544 \nTrain Batch      81 Loss : 0.6622812736917425 Time Taken : 0.21367553075154622 \nTrain Batch      86 Loss : 0.6674122682144475 Time Taken : 0.21107993125915528 \nTrain Batch      91 Loss : 0.6638958784905109 Time Taken : 0.2088297406832377 \nTrain Batch      96 Loss : 0.6642308418328563 Time Taken : 0.20870987176895142 \nTrain Batch     101 Loss : 0.6637407278660501 Time Taken : 0.21156394084294636 \nTrain Batch     106 Loss : 0.6682060110681461 Time Taken : 0.18532024224599203 \nTrain Batch     111 Loss : 0.6688701910478575 Time Taken : 0.18240561882654827 \nTrain Batch     116 Loss : 0.6666113510727882 Time Taken : 0.1862392743428548 \nTest Batch       1 Loss : 0.4062349200248718\nTest Batch       6 Loss : 0.5426149318615595\nTest Batch      11 Loss : 0.63110828128728\nTest Batch      16 Loss : 0.6655044760555029\nTest Batch      21 Loss : 0.6598893702030182\nTest Batch      26 Loss : 0.6755400219788918\nFor Epoch       8 Train Loss 0.669933 Train AUC 0.6124661246612466 Val Loss 0.66935113966465 Val AUC 0.6091920374707261 \nFor Epoch       8 Time Taken 44.972666192054746\nAUC Improved from 0.690145 to 0.66935113966465. Saved model to T1w-e8-loss0.669-auc0.609.pth\nRunning Epoch 9...................\nTrain Batch       1 Loss : 0.5986429452896118 Time Taken : 0.13446026643117268 \nTrain Batch       6 Loss : 0.6424464086691538 Time Taken : 0.20298086007436117 \nTrain Batch      11 Loss : 0.6342827352610502 Time Taken : 0.22129815419514973 \nTrain Batch      16 Loss : 0.6484207697212696 Time Taken : 0.21154865026473998 \nTrain Batch      21 Loss : 0.6623888640176683 Time Taken : 0.21922555367151897 \nTrain Batch      26 Loss : 0.6845914148367368 Time Taken : 0.20572787920633953 \nTrain Batch      31 Loss : 0.6892620940362254 Time Taken : 0.2079054315884908 \nTrain Batch      36 Loss : 0.6826418729292022 Time Taken : 0.21765127182006835 \nTrain Batch      41 Loss : 0.6822659860296947 Time Taken : 0.2073583165804545 \nTrain Batch      46 Loss : 0.6844272736621939 Time Taken : 0.2084469238917033 \nTrain Batch      51 Loss : 0.6851877248754689 Time Taken : 0.21290828386942545 \nTrain Batch      56 Loss : 0.6973006783851555 Time Taken : 0.2099424958229065 \nTrain Batch      61 Loss : 0.6913521587848663 Time Taken : 0.2126350482304891 \nTrain Batch      66 Loss : 0.6851517287167636 Time Taken : 0.2103700081507365 \nTrain Batch      71 Loss : 0.6841823643362018 Time Taken : 0.21198546489079792 \nTrain Batch      76 Loss : 0.6828691567245283 Time Taken : 0.21376094023386638 \nTrain Batch      81 Loss : 0.685391327481211 Time Taken : 0.20672552585601806 \nTrain Batch      86 Loss : 0.6910743664863498 Time Taken : 0.21893314917882284 \nTrain Batch      91 Loss : 0.6912873675535013 Time Taken : 0.2061461647351583 \nTrain Batch      96 Loss : 0.6889735028768579 Time Taken : 0.21138986349105834 \nTrain Batch     101 Loss : 0.6889639806629407 Time Taken : 0.2119705359141032 \nTrain Batch     106 Loss : 0.689550758813912 Time Taken : 0.17978866497675577 \nTrain Batch     111 Loss : 0.6924092331985096 Time Taken : 0.18743607600529988 \nTrain Batch     116 Loss : 0.6927285988269181 Time Taken : 0.18266580104827881 \nTest Batch       1 Loss : 0.8069769144058228\nTest Batch       6 Loss : 0.720094641049703\nTest Batch      11 Loss : 0.7318021926012906\nTest Batch      16 Loss : 0.7319772094488144\nTest Batch      21 Loss : 0.7408981805755979\nTest Batch      26 Loss : 0.7484999872170962\nFor Epoch       9 Train Loss 0.693427 Train AUC 0.5892294733758148 Val Loss 0.7630566895008087 Val AUC 0.3703161592505855 \nFor Epoch       9 Time Taken 50.52130915323893\nRunning Epoch 10...................\nTrain Batch       1 Loss : 0.5979776382446289 Time Taken : 0.13405027786890666 \nTrain Batch       6 Loss : 0.6485090255737305 Time Taken : 0.21605431636174519 \nTrain Batch      11 Loss : 0.6257947737520392 Time Taken : 0.20810664892196656 \nTrain Batch      16 Loss : 0.6419070586562157 Time Taken : 0.20826829274495443 \nTrain Batch      21 Loss : 0.6471955520766122 Time Taken : 0.20726321538289388 \nTrain Batch      26 Loss : 0.6570412585368524 Time Taken : 0.21225318908691407 \nTrain Batch      31 Loss : 0.6442016853440192 Time Taken : 0.21324277718861898 \nTrain Batch      36 Loss : 0.64454368998607 Time Taken : 0.20735810995101928 \nTrain Batch      41 Loss : 0.6428788529663552 Time Taken : 0.20960192680358886 \nTrain Batch      46 Loss : 0.650652690426163 Time Taken : 0.20296844641367595 \nTrain Batch      51 Loss : 0.6546807283279943 Time Taken : 0.20964330434799194 \nTrain Batch      56 Loss : 0.6583210192620754 Time Taken : 0.2141128698984782 \nTrain Batch      61 Loss : 0.6686524665746533 Time Taken : 0.20602924426396688 \nTrain Batch      66 Loss : 0.6779394054954703 Time Taken : 0.20807737112045288 \nTrain Batch      71 Loss : 0.6834118529944353 Time Taken : 0.21801971991856892 \nTrain Batch      76 Loss : 0.680963425651977 Time Taken : 0.21107714970906574 \nTrain Batch      81 Loss : 0.681599223687325 Time Taken : 0.21404678026835125 \nTrain Batch      86 Loss : 0.6805924929158632 Time Taken : 0.21070008277893065 \nTrain Batch      91 Loss : 0.6846924735294594 Time Taken : 0.20468372106552124 \nTrain Batch      96 Loss : 0.685273966131111 Time Taken : 0.20843791564305622 \nTrain Batch     101 Loss : 0.6795799322057479 Time Taken : 0.2071366310119629 \nTrain Batch     106 Loss : 0.679969589946405 Time Taken : 0.1843773086865743 \nTrain Batch     111 Loss : 0.6808854322712701 Time Taken : 0.18063155810038248 \nTrain Batch     116 Loss : 0.6786454440704708 Time Taken : 0.18020263115564983 \nTest Batch       1 Loss : 0.6796679496765137\nTest Batch       6 Loss : 0.6602639555931091\nTest Batch      11 Loss : 0.7128123153339733\nTest Batch      16 Loss : 0.7362482324242592\nTest Batch      21 Loss : 0.7337749032747178\nTest Batch      26 Loss : 0.7520702481269836\nFor Epoch      10 Train Loss 0.679143 Train AUC 0.6023584560169927 Val Loss 0.7463563283284506 Val AUC 0.4537470725995316 \nFor Epoch      10 Time Taken 56.01737096309662\n['T1w-e8-loss0.669-auc0.609.pth']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"def predict(modelfile, df, mri_type, split):\n    \n    print(\"Predict:\", modelfile, mri_type, df.shape)\n    df.loc[:,\"MRI_Type\"] = mri_type\n    \n    data_retriever = MRIScanDataset(\n        df.index.values, \n        mri_type=df[\"MRI_Type\"].values,\n        split=split\n    )\n\n    data_loader = DataLoader(\n        data_retriever,\n        batch_size=4,\n        shuffle=False,\n        num_workers=8,\n    )\n   \n    model = CNNModel()\n    model.to(device)\n    \n    checkpoint = torch.load(modelfile)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    \n    y_pred = []\n    ids = []\n\n    for e, batch in enumerate(data_loader,1):\n        print(f\"{e}/{len(data_loader)}\", end=\"\\r\")\n        with torch.no_grad():\n            tmp_pred = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n            if tmp_pred.size == 1:\n                y_pred.append(tmp_pred)\n            else:\n                y_pred.extend(tmp_pred.tolist())\n            ids.extend(batch[\"id\"].numpy().tolist())\n            \n    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred}) \n    preddf = preddf.set_index(\"BraTS21ID\")\n    return preddf","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:01:10.638579Z","iopub.execute_input":"2021-09-07T20:01:10.639002Z","iopub.status.idle":"2021-09-07T20:01:10.652569Z","shell.execute_reply.started":"2021-09-07T20:01:10.638951Z","shell.execute_reply":"2021-09-07T20:01:10.650968Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"model_files = ['./FLAIR-e1-loss0.661-auc0.647.pth', \n              './T1w-e8-loss0.669-auc0.609.pth',\n              './T1wCE-e3-loss0.699-auc0.505.pth',\n              './T2w-e4-loss0.644-auc0.696.pth']\n\nmri_types = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\ndf_valid = df_valid.set_index(\"BraTS21ID\")\n\nfor m, mtype in zip(model_files,  mri_types):\n    pred = predict(m, df_valid, mtype, \"train\")\n    df_valid[f\"MGMT_pred_{mtype}\"] = pred\n    \ndf_valid.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:01:49.089369Z","iopub.execute_input":"2021-09-07T20:01:49.089801Z","iopub.status.idle":"2021-09-07T20:04:33.355788Z","shell.execute_reply.started":"2021-09-07T20:01:49.089764Z","shell.execute_reply":"2021-09-07T20:04:33.354468Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Predict: ./FLAIR-e1-loss0.661-auc0.647.pth FLAIR (117, 2)\nPredict: ./T1w-e8-loss0.669-auc0.609.pth T1w (117, 3)\nPredict: ./T1wCE-e3-loss0.699-auc0.505.pth T1wCE (117, 4)\nPredict: ./T2w-e4-loss0.644-auc0.696.pth T2w (117, 5)\n30/30\r","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"           MGMT_value MRI_Type MGMT_pred_FLAIR MGMT_pred_T1w MGMT_pred_T1wCE  \\\nBraTS21ID                                                                      \n377                 0      T2w        0.395186      0.223082        0.197129   \n308                 0      T2w        0.356279      0.409498        0.535289   \n704                 1      T2w        0.578468        0.6553        0.551227   \n78                  1      T2w        0.562517      0.674889        0.537861   \n62                  1      T2w        0.649229      0.626686        0.525941   \n\n          MGMT_pred_T2w  \nBraTS21ID                \n377            0.412183  \n308            0.392604  \n704            0.726961  \n78             0.481782  \n62              0.69404  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MGMT_value</th>\n      <th>MRI_Type</th>\n      <th>MGMT_pred_FLAIR</th>\n      <th>MGMT_pred_T1w</th>\n      <th>MGMT_pred_T1wCE</th>\n      <th>MGMT_pred_T2w</th>\n    </tr>\n    <tr>\n      <th>BraTS21ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>377</th>\n      <td>0</td>\n      <td>T2w</td>\n      <td>0.395186</td>\n      <td>0.223082</td>\n      <td>0.197129</td>\n      <td>0.412183</td>\n    </tr>\n    <tr>\n      <th>308</th>\n      <td>0</td>\n      <td>T2w</td>\n      <td>0.356279</td>\n      <td>0.409498</td>\n      <td>0.535289</td>\n      <td>0.392604</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>1</td>\n      <td>T2w</td>\n      <td>0.578468</td>\n      <td>0.6553</td>\n      <td>0.551227</td>\n      <td>0.726961</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>1</td>\n      <td>T2w</td>\n      <td>0.562517</td>\n      <td>0.674889</td>\n      <td>0.537861</td>\n      <td>0.481782</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>1</td>\n      <td>T2w</td>\n      <td>0.649229</td>\n      <td>0.626686</td>\n      <td>0.525941</td>\n      <td>0.69404</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df_valid.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:06:33.497771Z","iopub.execute_input":"2021-09-07T20:06:33.498134Z","iopub.status.idle":"2021-09-07T20:06:33.508127Z","shell.execute_reply.started":"2021-09-07T20:06:33.498102Z","shell.execute_reply":"2021-09-07T20:06:33.506912Z"},"trusted":true},"execution_count":74,"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"MGMT_value           0\nMRI_Type             0\nMGMT_pred_FLAIR      0\nMGMT_pred_T1w        0\nMGMT_pred_T1wCE      0\nMGMT_pred_T2w        0\navg_pred           117\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df_valid['avg_pred'] = df_valid[['MGMT_pred_FLAIR', 'MGMT_pred_T1w', 'MGMT_pred_T1wCE', 'MGMT_pred_T2w']].mean(axis =1)\nroc_auc_score(df_valid['MGMT_value'], df_valid['avg_pred'])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:07:26.404778Z","iopub.execute_input":"2021-09-07T20:07:26.405152Z","iopub.status.idle":"2021-09-07T20:07:26.417748Z","shell.execute_reply.started":"2021-09-07T20:07:26.405119Z","shell.execute_reply":"2021-09-07T20:07:26.416584Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"0.6586651053864169"},"metadata":{}}]},{"cell_type":"code","source":"weights = [0.647, 0.609, 0.505, 0.696]\nweights = [i/sum(weights) for i in weights]\nweights","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:10:11.984050Z","iopub.execute_input":"2021-09-07T20:10:11.984594Z","iopub.status.idle":"2021-09-07T20:10:11.991004Z","shell.execute_reply.started":"2021-09-07T20:10:11.984559Z","shell.execute_reply":"2021-09-07T20:10:11.990040Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"[0.2633292633292634,\n 0.24786324786324787,\n 0.20553520553520555,\n 0.28327228327228327]"},"metadata":{}}]},{"cell_type":"code","source":"df_valid['weight_avg_pred'] = df_valid['MGMT_pred_FLAIR'] * weights[0] + df_valid['MGMT_pred_T1w'] * weights[1] + df_valid['MGMT_pred_T1wCE'] * weights[2] + df_valid['MGMT_pred_T2w'] * weights[3]\n    \nroc_auc_score(df_valid['MGMT_value'], df_valid['weight_avg_pred'])","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:11:51.328708Z","iopub.execute_input":"2021-09-07T20:11:51.329255Z","iopub.status.idle":"2021-09-07T20:11:51.547242Z","shell.execute_reply.started":"2021-09-07T20:11:51.329220Z","shell.execute_reply":"2021-09-07T20:11:51.546507Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"0.6651053864168619"},"metadata":{}}]},{"cell_type":"code","source":"model_files = ['./FLAIR-e1-loss0.661-auc0.647.pth', \n              './T1w-e8-loss0.669-auc0.609.pth',\n              './T1wCE-e3-loss0.699-auc0.505.pth',\n              './T2w-e4-loss0.644-auc0.696.pth']\n\nmri_types = ['FLAIR', 'T1w', 'T1wCE', 'T2w']\nsubmission = pd.read_csv(f\"{data_directory}/sample_submission.csv\", index_col=\"BraTS21ID\")\n\nfor m, mtype in zip(model_files,  mri_types):\n    pred = predict(m, submission, mtype, split=\"test\")\n    submission[f\"MGMT_pred_{mtype}\"] = pred\n    \nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:13:42.501859Z","iopub.execute_input":"2021-09-07T20:13:42.502269Z","iopub.status.idle":"2021-09-07T20:15:44.227656Z","shell.execute_reply.started":"2021-09-07T20:13:42.502222Z","shell.execute_reply":"2021-09-07T20:15:44.226221Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Predict: ./FLAIR-e1-loss0.661-auc0.647.pth FLAIR (87, 1)\nPredict: ./T1w-e8-loss0.669-auc0.609.pth T1w (87, 3)\nPredict: ./T1wCE-e3-loss0.699-auc0.505.pth T1wCE (87, 4)\nPredict: ./T2w-e4-loss0.644-auc0.696.pth T2w (87, 5)\n22/22\r","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"           MGMT_value MRI_Type  MGMT_pred_FLAIR  MGMT_pred_T1w  \\\nBraTS21ID                                                        \n1                 0.5      T2w         0.555620       0.578645   \n13                0.5      T2w         0.591802       0.647247   \n15                0.5      T2w         0.635078       0.604352   \n27                0.5      T2w         0.528732       0.612594   \n37                0.5      T2w         0.544178       0.644461   \n\n           MGMT_pred_T1wCE  MGMT_pred_T2w  \nBraTS21ID                                  \n1                 0.641596       0.444203  \n13                0.608111       0.450126  \n15                0.518244       0.572727  \n27                0.511286       0.598204  \n37                0.562171       0.692683  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MGMT_value</th>\n      <th>MRI_Type</th>\n      <th>MGMT_pred_FLAIR</th>\n      <th>MGMT_pred_T1w</th>\n      <th>MGMT_pred_T1wCE</th>\n      <th>MGMT_pred_T2w</th>\n    </tr>\n    <tr>\n      <th>BraTS21ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.5</td>\n      <td>T2w</td>\n      <td>0.555620</td>\n      <td>0.578645</td>\n      <td>0.641596</td>\n      <td>0.444203</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.5</td>\n      <td>T2w</td>\n      <td>0.591802</td>\n      <td>0.647247</td>\n      <td>0.608111</td>\n      <td>0.450126</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.5</td>\n      <td>T2w</td>\n      <td>0.635078</td>\n      <td>0.604352</td>\n      <td>0.518244</td>\n      <td>0.572727</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.5</td>\n      <td>T2w</td>\n      <td>0.528732</td>\n      <td>0.612594</td>\n      <td>0.511286</td>\n      <td>0.598204</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.5</td>\n      <td>T2w</td>\n      <td>0.544178</td>\n      <td>0.644461</td>\n      <td>0.562171</td>\n      <td>0.692683</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission1 = submission.copy()\nsubmission1['MGMT_value'] = submission[['MGMT_pred_FLAIR', 'MGMT_pred_T1w', 'MGMT_pred_T1wCE', 'MGMT_pred_T2w']].mean(axis =1)\nsubmission1['MGMT_value'].to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:24:40.715549Z","iopub.execute_input":"2021-09-07T20:24:40.715950Z","iopub.status.idle":"2021-09-07T20:24:40.726279Z","shell.execute_reply.started":"2021-09-07T20:24:40.715917Z","shell.execute_reply":"2021-09-07T20:24:40.724560Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"code","source":"submission2 = submission.copy()\nsubmission2['MGMT_value'] = submission['MGMT_pred_FLAIR'] * weights[0] + submission['MGMT_pred_T1w'] * weights[1] + submission['MGMT_pred_T1wCE'] * weights[2] + submission['MGMT_pred_T2w'] * weights[3]\nsubmission2['MGMT_value'].to_csv('submission_weighted_avg.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:18:34.307992Z","iopub.execute_input":"2021-09-07T20:18:34.308551Z","iopub.status.idle":"2021-09-07T20:18:34.319885Z","shell.execute_reply.started":"2021-09-07T20:18:34.308515Z","shell.execute_reply":"2021-09-07T20:18:34.318720Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"submission3 = submission.copy()\nsubmission3['MGMT_value'] = submission['MGMT_pred_T2w']\nsubmission3['MGMT_value'].to_csv('submission_only_using_T2W.csv')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T20:19:14.777215Z","iopub.execute_input":"2021-09-07T20:19:14.777665Z","iopub.status.idle":"2021-09-07T20:19:14.787545Z","shell.execute_reply.started":"2021-09-07T20:19:14.777625Z","shell.execute_reply":"2021-09-07T20:19:14.786399Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save model state dict\n# model = CNNModel()\n# PATH = './FLAIR-e1-loss0.661-auc0.647.pth'\n# checkpoint = torch.load(PATH)\n# model.load_state_dict(checkpoint[\"model_state_dict\"])\n# torch.save(model.state_dict(), './FLAIR_model_state_dict')","metadata":{"execution":{"iopub.status.busy":"2021-09-07T19:58:47.125213Z","iopub.execute_input":"2021-09-07T19:58:47.125593Z","iopub.status.idle":"2021-09-07T19:58:47.951360Z","shell.execute_reply.started":"2021-09-07T19:58:47.125550Z","shell.execute_reply":"2021-09-07T19:58:47.949898Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-09-07T19:24:30.190632Z","iopub.execute_input":"2021-09-07T19:24:30.190981Z","iopub.status.idle":"2021-09-07T19:24:30.416403Z","shell.execute_reply.started":"2021-09-07T19:24:30.190951Z","shell.execute_reply":"2021-09-07T19:24:30.415572Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid[\"MGMT_pred\"] /= len(modelfiles)\nauc = roc_auc_score(df_valid[\"MGMT_value\"], df_valid[\"MGMT_pred\"])\nprint(f\"Validation ensemble AUC: {auc:.4f}\")\nsns.displot(df_valid[\"MGMT_pred\"])","metadata":{},"execution_count":null,"outputs":[]}]}